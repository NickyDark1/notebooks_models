{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOhkBvFp11DKfsdKhjGjgFl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/stablebeluga_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget https://huggingface.co/TheBloke/StableBeluga-13B-GGML/resolve/main/stablebeluga-13b.ggmlv3.q6_K.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVU9c9b2daSb",
        "outputId": "349161ba-72a3-440d-8522-8cedec7924a5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-29 04:18:10--  https://huggingface.co/TheBloke/StableBeluga-13B-GGML/resolve/main/stablebeluga-13b.ggmlv3.q6_K.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.224.249.43, 13.224.249.119, 13.224.249.10, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.224.249.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/78/a9/78a9edd70b7383e401f5fad995be8125c0360b874fcc97b30a9baf51366db065/59e1f74d30bdce1995c9c94901e75bc32e34e31dd8e8f1ab1cda6ea0f18bb54d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27stablebeluga-13b.ggmlv3.q6_K.bin%3B+filename%3D%22stablebeluga-13b.ggmlv3.q6_K.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1690863491&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MDg2MzQ5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83OC9hOS83OGE5ZWRkNzBiNzM4M2U0MDFmNWZhZDk5NWJlODEyNWMwMzYwYjg3NGZjYzk3YjMwYTliYWY1MTM2NmRiMDY1LzU5ZTFmNzRkMzBiZGNlMTk5NWM5Yzk0OTAxZTc1YmMzMmUzNGUzMWRkOGU4ZjFhYjFjZGE2ZWEwZjE4YmI1NGQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=zKlcYw4y1DGbfxM68YFznLFUKr-IIyrloJZ89D1fN7fgA3ErVPe8JQGACLZkeKBqsAUcDrAIOX8oDvhoYiYtkMy5lyqViiw1or98OYCrIjpcogTGs1HikQFuP6H0zEkt6Vu6NsiqHE2qcr12RX-6N8BCWkFIbYax4wlPVEqC32I3FXz7WNk-d2yHZSYTr9MFZQNRtNOn8CPvpew4LkQeM-NJ0zrMuvdoHDD-A99E4DUyGAi3ZkI8tq7%7EA3BdNEdS%7EQxIHpUozdlkhkYk3vZKfowZQhCzSc0RCWKP3%7EFhrcD2TlHej0m0yJQwoYuoxUC3TmfuqTYG9CWNXiAeY8CakQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-07-29 04:18:11--  https://cdn-lfs.huggingface.co/repos/78/a9/78a9edd70b7383e401f5fad995be8125c0360b874fcc97b30a9baf51366db065/59e1f74d30bdce1995c9c94901e75bc32e34e31dd8e8f1ab1cda6ea0f18bb54d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27stablebeluga-13b.ggmlv3.q6_K.bin%3B+filename%3D%22stablebeluga-13b.ggmlv3.q6_K.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1690863491&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MDg2MzQ5MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83OC9hOS83OGE5ZWRkNzBiNzM4M2U0MDFmNWZhZDk5NWJlODEyNWMwMzYwYjg3NGZjYzk3YjMwYTliYWY1MTM2NmRiMDY1LzU5ZTFmNzRkMzBiZGNlMTk5NWM5Yzk0OTAxZTc1YmMzMmUzNGUzMWRkOGU4ZjFhYjFjZGE2ZWEwZjE4YmI1NGQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=zKlcYw4y1DGbfxM68YFznLFUKr-IIyrloJZ89D1fN7fgA3ErVPe8JQGACLZkeKBqsAUcDrAIOX8oDvhoYiYtkMy5lyqViiw1or98OYCrIjpcogTGs1HikQFuP6H0zEkt6Vu6NsiqHE2qcr12RX-6N8BCWkFIbYax4wlPVEqC32I3FXz7WNk-d2yHZSYTr9MFZQNRtNOn8CPvpew4LkQeM-NJ0zrMuvdoHDD-A99E4DUyGAi3ZkI8tq7%7EA3BdNEdS%7EQxIHpUozdlkhkYk3vZKfowZQhCzSc0RCWKP3%7EFhrcD2TlHej0m0yJQwoYuoxUC3TmfuqTYG9CWNXiAeY8CakQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.94, 18.155.68.73, 18.155.68.128, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10678850688 (9.9G) [application/octet-stream]\n",
            "Saving to: ‘stablebeluga-13b.ggmlv3.q6_K.bin’\n",
            "\n",
            "stablebeluga-13b.gg 100%[===================>]   9.95G   231MB/s    in 60s     \n",
            "\n",
            "2023-07-29 04:19:11 (170 MB/s) - ‘stablebeluga-13b.ggmlv3.q6_K.bin’ saved [10678850688/10678850688]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HsI0XL-eZ6E",
        "outputId": "a91f0597-cf1c-43b5-a858-522c6da50621"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.1.77.tar.gz (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m196.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m178.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.77-cp310-cp310-linux_x86_64.whl size=1368669 sha256=e6deb483e16c8f33dcbb9c530f465d3430978ad7b7323c8b30a05505402bd5df\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bfrn1wli/wheels/aa/ed/39/87f2ad350dbbf13b600ac744899186b8647c5323c62e2bb348\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.1 llama-cpp-python-0.1.77 numpy-1.25.1 typing-extensions-4.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4wyXBJ554Zq",
        "outputId": "7253b84f-4804-4fc0-afa6-3e50739585f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "IRIpqPyR54e5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n"
      ],
      "metadata": {
        "id": "cpdsNpk258Rg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 1000  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"stablebeluga-13b.ggmlv3.q6_K.bin\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        "    n_ctx=1000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyckGKMn58UQ",
        "outputId": "3870bcd8-1ac6-4c24-f623-39d9aa9fa057"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUdJpjq658Y4",
        "outputId": "74602669-1e51-4d0b-b593-64f15005c8a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "    \"Question: What are the names of the planets in the solar system? Answer: \", stop=['Question:']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptKjyiOX9Z8k",
        "outputId": "e26f5e50-5a3f-4137-9643-c138a5df502f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Mercury 2) Venus 3) Earth (our planet!) 4) Mars 5) Jupiter 6) Saturn 7) Uranus 8) Neptune 9) Pluto (but some people say it's not a planet anymore because it doesn't orbit the sun like all of the other planets do)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm('''\n",
        "use the following information and answer the question\n",
        "A large language model (LLM) is a language model characterized by emergent properties enabled by its large size.[1] They are built with artificial neural networks, (pre-)trained using self-supervised learning and semi-supervised learning, typically containing tens of millions to billions of weights. They are trained using specialized AI accelerator hardware to parallel process vast amounts of text data, mostly scraped from the Internet. As language models, they work by taking an input text and repeatedly predicting the next token or word.[2] The 2017 invention of the transformer architecture drove a series of breakthroughs in LLM development.[3] Older, specialized supervised models for specific linguistic tasks, have been made largely obsolete by the emergent abilities of LLMs,[4] which are thought to acquire embodied knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[4] Notable LLMs include GPT-4, LLaMa, PaLM, BLOOM, Ernie 3.0 Titan, and Claude.\n",
        "Question: Summarize the above\n",
        "    ''', stop=['Question:'])"
      ],
      "metadata": {
        "id": "4yJEhT5LBxuf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "9e439231-da2a-4210-a3bf-d0ccf382e3e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Large language models (LLMs) are large neural networks trained using self-supervised and semi-supervised learning techniques, typically containing tens of millions to billions of weights. They process vast amounts of text data from the internet and can predict the next token or word in a text. The transformer architecture revolutionized LLM development in 2017, making specialized supervised models for specific linguistic tasks obsolete. Although LLMs acquire embodied knowledge about human language corpora, they also inherit biases and inaccuracies present in the data. Notable examples of LLMs include GPT-4, LLaMa, PaLM, BLOOM, Ernie 3.0 Titan, and Claude."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Large language models (LLMs) are large neural networks trained using self-supervised and semi-supervised learning techniques, typically containing tens of millions to billions of weights. They process vast amounts of text data from the internet and can predict the next token or word in a text. The transformer architecture revolutionized LLM development in 2017, making specialized supervised models for specific linguistic tasks obsolete. Although LLMs acquire embodied knowledge about human language corpora, they also inherit biases and inaccuracies present in the data. Notable examples of LLMs include GPT-4, LLaMa, PaLM, BLOOM, Ernie 3.0 Titan, and Claude.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm('''\n",
        "Write a story about love\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "R4hJtP04y3X1",
        "outputId": "5555514c-4773-4886-8a91-8434c589f0f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, there was a small village nestled among the hills. In this quaint little village, lived two young people who were deeply in love with each other. Their names were John and Mary. They spent every day together, sharing their dreams, hopes and laughter. Everyone in the village knew that these two belonged together and would soon marry.\n",
            "One day, a mysterious traveler arrived in the village. His name was Jameson and he claimed to be a fortune teller who could see into people's futures. All of the villagers were intrigued by Jameson's claims, including John and Mary. They decided to visit Jameson together, hoping he would give them a glimpse of their future.\n",
            "As they sat down with the fortune teller, Jameson's eyes grew wide with wonder as he stared at John and Mary. \"I see two hearts beating as one,\" he said with a smile, \"Your love for each other is truly special.\" They both smiled, feeling warmth and comfort from his words.\n",
            "Jameson continued, \"You are destined to be together forever, but you must face some trials along the way.\" John and Mary's smiles"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Once upon a time, there was a small village nestled among the hills. In this quaint little village, lived two young people who were deeply in love with each other. Their names were John and Mary. They spent every day together, sharing their dreams, hopes and laughter. Everyone in the village knew that these two belonged together and would soon marry.\\nOne day, a mysterious traveler arrived in the village. His name was Jameson and he claimed to be a fortune teller who could see into people\\'s futures. All of the villagers were intrigued by Jameson\\'s claims, including John and Mary. They decided to visit Jameson together, hoping he would give them a glimpse of their future.\\nAs they sat down with the fortune teller, Jameson\\'s eyes grew wide with wonder as he stared at John and Mary. \"I see two hearts beating as one,\" he said with a smile, \"Your love for each other is truly special.\" They both smiled, feeling warmth and comfort from his words.\\nJameson continued, \"You are destined to be together forever, but you must face some trials along the way.\" John and Mary\\'s smiles'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['SERPAPI_API_KEY'] ='9839040d68ff8b897445dcb7595405cdc09826b0daac52888b91cbfb18c03456'\n",
        "\n",
        "#api key available at https://serpapi.com/manage-api-key"
      ],
      "metadata": {
        "id": "IQyQPAZ75f4M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType"
      ],
      "metadata": {
        "id": "l3ZKyKx67GXz"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"serpapi\"], llm=llm)\n",
        "\n"
      ],
      "metadata": {
        "id": "sE9JhHjF7LQ1"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
      ],
      "metadata": {
        "id": "2V4wYc3A7Mvs"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Who is the president of USA?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "QF3n_GzE7OX_",
        "outputId": "f15736ae-e5af-4be9-ec31-f1c86a70e2da"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I should search for information about the current president of the United States.\n",
            "Action: Search\n",
            "Action Input: \"who is the president of the united states\"\u001b[32;1m\u001b[1;3m I should search for information about the current president of the United States.\n",
            "Action: Search\n",
            "Action Input: \"who is the president of the united states\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mhttps://www.whitehouse.gov/about-the-white-house/presidents/\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I now know the final answer\n",
            "Final Answer: Joe Biden\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Joe Biden\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Joe Biden'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"What happened in congress hearing of uap?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "IQ7aZeFE7Ry-",
        "outputId": "f29041d8-2e2e-4ff2-b4a1-69d00c1318dd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I should look up the recent news on congressional hearings.\n",
            "Action: Search\n",
            "Action Input: \"congress hearing uap\"\u001b[32;1m\u001b[1;3m I should look up the recent news on congressional hearings.\n",
            "Action: Search\n",
            "Action Input: \"congress hearing uap\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mA former military intelligence officer-turned-whistleblower told House lawmakers that Congress is being kept in the dark about unidentified ...\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I can find more information on this topic.\n",
            "Action: Search\n",
            "Action Input: \"uap congress hearing\"\u001b[32;1m\u001b[1;3m I can find more information on this topic.\n",
            "Action: Search\n",
            "Action Input: \"uap congress hearing\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThree retired military veterans testified Wednesday at a House hearing on unidentified anomalous phenomena – commonly known as UFOs ...\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The search results suggest that the hearing may be focused on UAP, not UFOs.\n",
            "Action: Search\n",
            "Action Input: \"uap congress hearing\"\u001b[32;1m\u001b[1;3m The search results suggest that the hearing may be focused on UAP, not UFOs.\n",
            "Action: Search\n",
            "Action Input: \"uap congress hearing\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThree retired military veterans testified Wednesday at a House hearing on unidentified anomalous phenomena – commonly known as UFOs ...\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The search results suggest that the hearing is about UAP and UFOs.\n",
            "Final Answer: The recent congressional hearing was focused on unidentified anomalous phenomena (UAP) and commonly known as UFOs.\u001b[32;1m\u001b[1;3m The search results suggest that the hearing is about UAP and UFOs.\n",
            "Final Answer: The recent congressional hearing was focused on unidentified anomalous phenomena (UAP) and commonly known as UFOs.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The recent congressional hearing was focused on unidentified anomalous phenomena (UAP) and commonly known as UFOs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"What is the capital of India?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "2fgEaP089BgH",
        "outputId": "9ce98083-379a-406f-f229-b9923a25e5be"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I should search for the capital of India.\n",
            "Action: Search\n",
            "Action Input: \"capital of India\"\u001b[32;1m\u001b[1;3m I should search for the capital of India.\n",
            "Action: Search\n",
            "Action Input: \"capital of India\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mNew Delhi\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I now know the final answer.\n",
            "Final Answer: New Delhi\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: New Delhi\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'New Delhi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZvvEGkst__1H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}