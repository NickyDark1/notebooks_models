{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b29478-71b5-407f-9220-201532505e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1699b768-3597-4db6-9897-ebc6ec98c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, inspect, json, re\n",
    "import xml.etree.ElementTree as ET\n",
    "from functools import partial\n",
    "from typing import get_type_hints\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from langchain.chains.openai_functions import convert_to_openai_function\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14e6a7e-3686-4799-8211-860404615419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/workspace/LLaMA-Factory/qwen-1.5-0.5B-tool\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed06ace-2b8b-4d1f-bf43-9b1387af671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, local_files_only=True)\n",
    "\n",
    "    with torch.device(\"cuda:0\"):\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).eval()\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99638ae-287a-4f4f-a623-6e9aad98e9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6c475ab-417e-4df6-b6f2-c9b48957aaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f529965309b647518683d1a43aeb9c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f253579-8a3b-451b-8f1e-f7912b75e5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Neuranest/Qwen-1.5-0.5B-toolcall/commit/4788f69ca958778a838760d223f6de8694b421d3', commit_message='Upload Qwen2ForCausalLM', commit_description='', oid='4788f69ca958778a838760d223f6de8694b421d3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Neuranest/Qwen-1.5-0.5B-toolcall\", token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "509aeb7e-0175-41bf-baea-767a938155e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475b9dd5c8f745628cae34ac57d1b2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Neuranest/Qwen-1.5-0.5B-toolcall/commit/714b25417881be8fdc15f024cee3e3fe5d6d271c', commit_message='Upload tokenizer', commit_description='', oid='714b25417881be8fdc15f024cee3e3fe5d6d271c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"Neuranest/Qwen-1.5-0.5B-toolcall\", token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75b40ead-70aa-46a7-87c7-82525cb5ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Neuranest/Qwen-1.5-0.5B-toolcall\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6c61b60-5abc-4edd-a2b4-c5f326f042e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db092a0daab24bc1a80cfdd82063da33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d252409d7f954355885224565bd381c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ba0c677fa846edad6705367fc27b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6db901f6bd493b9c2c3e9e47ef9f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1534802ac2cc424a9aaeabc5ed6d4bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675f0a5997fe421db129d4d927af8604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/370 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27249e8624d24282a180f0128d06e92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f634c156e04712bd38caeaa8bd60d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/928M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d70c48d69c43198cd2dbd278a0a56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/199 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (3048522922.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[45], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    return tokenizer, model\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "with torch.device(\"cuda:0\"):\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c964aa-cf58-4a99-a8e0-7dea14ef6c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/runpy.py:126: RuntimeWarning: 'llmtuner.api.app' found in sys.modules after import of package 'llmtuner.api', but prior to execution of 'llmtuner.api.app'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 17:22:38,825 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 17:22:38,825 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 17:22:38,825 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 17:22:38,825 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 17:22:38,825 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 17:22:38,825 >> loading file tokenizer.json\n",
      "[WARNING|logging.py:314] 2024-02-15 17:22:39,016 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:727] 2024-02-15 17:22:39,017 >> loading configuration file /workspace/LLaMA-Factory/qwen-1.5-0.5B-tool/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 17:22:39,018 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/workspace/LLaMA-Factory/qwen-1.5-0.5B-tool\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3473] 2024-02-15 17:22:39,177 >> loading weights file /workspace/LLaMA-Factory/qwen-1.5-0.5B-tool/model.safetensors\n",
      "[INFO|modeling_utils.py:1426] 2024-02-15 17:22:39,185 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 17:22:39,186 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4350] 2024-02-15 17:22:39,751 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-15 17:22:39,751 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /workspace/LLaMA-Factory/qwen-1.5-0.5B-tool.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:779] 2024-02-15 17:22:39,753 >> loading configuration file /workspace/LLaMA-Factory/qwen-1.5-0.5B-tool/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 17:22:39,753 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "02/15/2024 17:22:39 - INFO - llmtuner.model.adapter - Adapter is not found at evaluation, load the base model.\n",
      "02/15/2024 17:22:39 - INFO - llmtuner.model.loader - trainable params: 0 || all params: 463987712 || trainable%: 0.0000\n",
      "02/15/2024 17:22:39 - INFO - llmtuner.model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2163\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38962 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38976 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38976 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53796 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:37858 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:37838 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:37838 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:46342 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:46342 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47436 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m llmtuner.api.app --model_name_or_path /workspace/LLaMA-Factory/qwen-1.5-0.5B-tool --template default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98891caa-17c9-4060-bbbf-c476f1c576d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f66bc1-849b-4b1c-b00a-d77a7c16dc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
