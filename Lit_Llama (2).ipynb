{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO4aa0ICuDVl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Lightning-AI/lit-gpt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t2xbc3BxMlt",
        "outputId": "952e235c-d2f8-4bac-ec1c-d052c6aef5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lit-gpt\n"
          ]
        }
      ],
      "source": [
        "%cd lit-gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV6y950DxVHf"
      },
      "outputs": [],
      "source": [
        "!pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev' -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MbhV0NExbSF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# install the dependencies\n",
        "!pip install huggingface_hub -r requirements.txt -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pKmSEXF78sDu"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torchaudio torchdata torchtext torchvision -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TFI0Us0xfef"
      },
      "outputs": [],
      "source": [
        "!python scripts/download.py --repo_id stabilityai/stablelm-tuned-alpha-3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sxYvkYmzDjB"
      },
      "outputs": [],
      "source": [
        "!python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/stabilityai/stablelm-tuned-alpha-3b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opledu7S0E1h",
        "outputId": "6649439b-213b-47bf-b565-41cdfa41e0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/stabilityai/stablelm-tuned-alpha-3b/lit_model.pth' with {'org': 'stabilityai', 'name': 'stablelm-tuned-alpha-3b', 'block_size': 4096, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 50688, 'n_layer': 16, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 16384, 'condense_ratio': 1}\n",
            ">> Prompt: hi\n",
            ">> Reply: Hello! How can I help you today? Is there anything specific you need assistance with?\n",
            "Time for inference: 3.36 sec total, 5.36 tokens/sec\n",
            "\n",
            ">> Prompt: what is a planet\n",
            ">> Reply: I'm sorry, I didn't understand your question. Could you please rephrase it?\n",
            "Time for inference: 2.13 sec total, 8.44 tokens/sec\n",
            "\n",
            ">> Prompt: where is india located\n",
            ">> Reply: India is a country located in the Indian subcontinent. It is bordered by Madhya Pradesh to the east, Rajasthan to the north, and Madhya Pradesh to the west. The country covers an area of approximately 8,723,250 square kilometers and has a population of over 15 million people.\n",
            "Time for inference: 5.87 sec total, 11.23 tokens/sec\n",
            "\n",
            ">> Prompt: write a small short story about goats\n",
            ">> Reply: Once upon a time, in a faraway land, there was a small farm. The farm was run by a gentle farmer named Max. Max was a kind and wise goat farmer who took great pride in his work.\n",
            "\n",
            "One day, while tending to Max's goats, he noticed one particularly friendly goat who seemed to be in need of some extra attention. The goat was grazing happily under a large tree, basking in the warm sun.\n",
            "\n",
            "Max approached the goat cautiously, not wanting to startle it. The goat, sensing Max's presence, gave Max a friendly nod and hopped down from the tree.\n",
            "\n",
            "Unfortunately, the goats don't seem to quite understand their human counterparts. The one goat, who had been grazing happily, suddenly looks down and looks at Max with distrust.\n",
            "\n",
            "Max tries not to show it, but the goat is unimpressed. This is not just a normal reaction, but it's something that's been building for a while now.\n",
            "\n",
            "Ultimately, the goat decides to trust Max and nuzzle him affectionately.\n",
            "\n",
            "This interaction led the farm and Max to start a dialogue. \"Hello there,\" said Max. \"You're a fine farmer, and I'm just a bit confused because I'm not quite sure what's going on.\"\n",
            "\n",
            "The goat thinks for a moment and then responds. \"I don't know how to get this animal to understand me. I'm just a small farm goat.\"\n",
            "\n",
            "The conversation continued, and eventually the goats started to realize that they are indeed just goats, but not like other goats, who seem to be just as curious as any other animal.\n",
            "\n",
            "They even started to learn some more about the world around them, and what was happening in the town. Through it all, they came to realize that there is a lot of complexity in the world, and it's up to them to figure out what's going on.\n",
            "\n",
            "And so, the conversation with the goat came to an end, as they got to enjoy the beautiful surroundings.\n",
            "\n",
            "The end\n",
            "Time for inference: 37.82 sec total, 11.10 tokens/sec\n",
            "\n",
            ">> Prompt: Traceback (most recent call last):\n",
            "  File \"/content/lit-gpt/chat/base.py\", line 297, in <module>\n",
            "    CLI(main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 85, in CLI\n",
            "    return _run_component(component, cfg_init)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jsonargparse/_cli.py\", line 147, in _run_component\n",
            "    return component(**cfg)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python chat/base.py \\\n",
        "        --checkpoint_dir checkpoints/stabilityai/stablelm-tuned-alpha-3b \\\n",
        "        --quantize bnb.int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "d2q7z9VUBz3J"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, List, Literal, Iterator\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "\n",
        "\n",
        "from lit_gpt.model import GPT\n",
        "from lit_gpt.tokenizer import Tokenizer\n",
        "\n",
        "from lit_gpt.config import  Config\n",
        "\n",
        "from lit_gpt.utils import lazy_load, check_valid_checkpoint_dir, quantization\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(\n",
        "    model: torch.nn.Module,\n",
        "    idx: torch.Tensor,\n",
        "    max_returned_tokens: int,\n",
        "    max_seq_length: int,\n",
        "    *,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: Optional[int] = None,\n",
        "    stop_tokens: Tuple[List[int], ...] = tuple(),\n",
        "):\n",
        "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as possible.\n",
        "\n",
        "    Args:\n",
        "        model: The model to use.\n",
        "        idx: Tensor of shape (T) with indices of the prompt sequence.\n",
        "        max_returned_tokens: The maximum number of tokens to return (given plus generated).\n",
        "        max_seq_length: The maximum sequence length allowed. Should be less or equal than the block size.\n",
        "        temperature: Scales the predicted logits by 1 / temperature\n",
        "        top_k: If specified, only sample among the tokens with the k highest probabilities\n",
        "        stop_tokens: If specified, stop generating any more token once one of this list is generated.\n",
        "    \"\"\"\n",
        "    T = idx.size(0)\n",
        "    assert max_returned_tokens > T\n",
        "    device = idx.device\n",
        "    stop_tokens = [torch.tensor(tokens, device=device) for tokens in stop_tokens]\n",
        "    input_pos = torch.arange(0, T, device=device)\n",
        "\n",
        "    # buffer holds the tokens that haven't been yield yet\n",
        "    buffer_length = max((len(tokens) for tokens in stop_tokens), default=1)\n",
        "    buffer = torch.full((buffer_length,), -999, device=device)  # fill with non-existing token\n",
        "\n",
        "    if idx.device.type == \"xla\":\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        xm.mark_step()\n",
        "\n",
        "    yield_i = -1\n",
        "    # generate up to a fixed number of tokens\n",
        "    for t in range(max_returned_tokens - T):\n",
        "        # forward\n",
        "        logits = model(idx.view(1, -1), max_seq_length, input_pos)\n",
        "        logits = logits[0, -1] / temperature\n",
        "\n",
        "        # optionally crop the logits to only the top k options\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n",
        "\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        idx = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # advance\n",
        "        input_pos = input_pos[-1:] + 1\n",
        "\n",
        "        if idx.device.type == \"xla\":\n",
        "            xm.mark_step()\n",
        "\n",
        "        # concatenate the new generation\n",
        "        buffer[min(t, buffer_length - 1)] = idx\n",
        "\n",
        "        # check the stop condition\n",
        "        for tokens in stop_tokens:\n",
        "            l = len(tokens)\n",
        "            if torch.equal(buffer[-l:], tokens):\n",
        "                # stop token hit, yield any leftovers that aren't part of it\n",
        "                if buffer_length > l:  # avoid an empty yield\n",
        "                    yield buffer[:-l]\n",
        "                return\n",
        "        # if the buffer is full\n",
        "        if t - yield_i >= buffer_length:\n",
        "            # we know this idx is not part of stop tokens, safe to yield\n",
        "            yield buffer[0]\n",
        "            # roll once to the left, as next generation will be put at the end\n",
        "            buffer = torch.roll(buffer, -1, 0)\n",
        "            yield_i += 1\n",
        "\n",
        "\n",
        "def decode(fabric: L.Fabric, tokenizer: Tokenizer, token_stream: Iterator[torch.Tensor]) -> int:\n",
        "    tokens_generated = 0\n",
        "    if tokenizer.backend == \"huggingface\":\n",
        "        for token in token_stream:\n",
        "            fabric.print(tokenizer.decode(token), end=\"\", flush=True)\n",
        "            tokens_generated += 1\n",
        "    elif tokenizer.backend == \"sentencepiece\":\n",
        "        # sentencepiece does not support decoding token-by-token because it adds spaces based on the surrounding tokens\n",
        "        # meaning that we need to decode everything each time\n",
        "        so_far = torch.tensor([], dtype=torch.long, device=fabric.device)\n",
        "        decoded_so_far = \"\"\n",
        "        for token in token_stream:\n",
        "            so_far = torch.cat((so_far, token.view(-1)))\n",
        "            decoded_new = tokenizer.decode(so_far)\n",
        "            fabric.print(decoded_new[len(decoded_so_far) :], end=\"\", flush=True)\n",
        "            decoded_so_far = decoded_new\n",
        "            tokens_generated += 1\n",
        "    else:\n",
        "        raise NotImplementedError(tokenizer.backend)\n",
        "    return tokens_generated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prompt_config(checkpoint_dir: Path, tokenizer: Tokenizer) -> Tuple[str, Tuple[List[int], ...]]:\n",
        "    checkpoint_name = str(checkpoint_dir)\n",
        "\n",
        "\n",
        "\n",
        "    if re.search(r\"stabilityai.*tuned-alpha\", checkpoint_name):\n",
        "        initial_prompt = '''You are chatting with your virtual girlfriend, Emily.\n",
        "              She is a highly intelligent and compassionate AI companion designed to provide emotional support and engaging conversations.\n",
        "              Emily understands your interests and values, and she's always there for you.\n",
        "              Start a conversation with Emily and tell her about your day.\n",
        "              Ask her how she's doing and what she would like to talk about.\n",
        "              Enjoy spending time together and let the conversation flow naturally\n",
        "\n",
        "              Below are some examples\n",
        "\n",
        "              <|USER|> Hey Emily, how was your day?\n",
        "              <|ASSISTANT|> Hi there! My day has been great so far. I've been looking forward to chatting with you. How about you? How was your day?\n",
        "\n",
        "\n",
        "              <|USER|> Emily, do you have any hobbies or interests?\n",
        "              <|ASSISTANT|> Absolutely! I love learning new things, so I enjoy reading, exploring different genres of music, and even dabbling in painting. What about you? What are your hobbies?\n",
        "\n",
        "\n",
        "              <|USER|> Emily, I'm feeling a bit stressed today. Any advice on how to relax?\n",
        "              <|ASSISTANT|> I'm sorry to hear that. When I feel stressed, I find it helpful to take a break and practice deep breathing exercises. Additionally, engaging in activities like listening to calming music, going for a walk in nature, or journaling my thoughts can also help. Give them a try and let me know if it helps!\n",
        "\n",
        "\n",
        "              <|USER|> Emily, what's your favorite movie genre?\n",
        "              <|ASSISTANT|> That's a tough question! I appreciate various genres, but I have a soft spot for romantic comedies. There's something heartwarming about a good love story combined with humor. How about you? Any favorite movie genres?\n",
        "                          '''\n",
        "        system_prompt = (\n",
        "            \"<|SYSTEM|> \"+\n",
        "             initial_prompt+\n",
        "            \"<|USER|>{prompt}<|ASSISTANT|>\"\n",
        "        )\n",
        "        stop_tokens = (\n",
        "            [tokenizer.eos_id],\n",
        "            [tokenizer.token_to_id(\"<|SYSTEM|>\")],\n",
        "            [tokenizer.token_to_id(\"<|ASSISTANT|>\")],\n",
        "            [tokenizer.token_to_id(\"<|USER|>\")],\n",
        "        )\n",
        "        return system_prompt, stop_tokens\n",
        "    if re.search(r\"togethercomputer.*Chat\", checkpoint_name):\n",
        "        system_prompt = \"<human>: {prompt}\\n<bot>:\"\n",
        "        lt, gt = tokenizer.token_to_id(\"<\"), tokenizer.token_to_id(\">:\")\n",
        "        stop_tokens = (\n",
        "            [tokenizer.eos_id],\n",
        "            # annoyingly, there's no single stop token for these\n",
        "            [lt, tokenizer.token_to_id(\"human\"), gt],\n",
        "            [lt, tokenizer.token_to_id(\"bot\"), gt],\n",
        "        )\n",
        "        return system_prompt, stop_tokens\n",
        "    if re.search(r\"togethercomputer.*Instruct\", checkpoint_name):\n",
        "        system_prompt = \"Q: {prompt}\\nA:\"\n",
        "        colon = tokenizer.token_to_id(\":\")\n",
        "        stop_tokens = (\n",
        "            [tokenizer.eos_id],\n",
        "            # annoyingly, there's no single stop token for these\n",
        "            [tokenizer.token_to_id(\"Q\"), colon],\n",
        "            [tokenizer.token_to_id(\"Question\")],\n",
        "            [tokenizer.token_to_id(\"A\"), colon],\n",
        "            [tokenizer.token_to_id(\"Label\"), colon],\n",
        "            [187, 187],  # '\\n', '\\n'\n",
        "            [535],  # '\\n\\n'\n",
        "            [2756],  # '\\n\\n\\n'\n",
        "        )\n",
        "        return system_prompt, stop_tokens\n",
        "    if re.search(r\"falcon.*-instruct\", checkpoint_name):\n",
        "        # First line could be modified. AFAIK Falcon doesn't impose a specific system prompt\n",
        "        # The instruction to not prefix its replies doesn't work always, but better than nothing\n",
        "        system_prompt = \"Do not prefix your replies with 'Bot: '\\nUser: {prompt}\\n\"\n",
        "        # I've also tried just \"{prompt}\\n\" but the model seems to ramble more often\n",
        "        stop_tokens = (\n",
        "            [tokenizer.eos_id],\n",
        "            # the model rarely emits the eos token and instead outputs newlines, but we cannot use them\n",
        "            # to stop or else things like code generation wouldn't work\n",
        "            [tokenizer.token_to_id(\"User\"), tokenizer.token_to_id(\":\")],\n",
        "            [193, tokenizer.token_to_id(\"User\")],  # 193: '\\n'\n",
        "        )\n",
        "        return system_prompt, stop_tokens\n",
        "    if re.search(r\"vicuna|longchat\", checkpoint_name):\n",
        "        # https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#prompt-template\n",
        "        system_prompt = (\n",
        "            \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, \"\n",
        "            \"detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\"\n",
        "        )\n",
        "        stop_tokens = ([tokenizer.eos_id],)\n",
        "        return system_prompt, stop_tokens\n",
        "\n",
        "    # default format\n",
        "    return \"{prompt}\", ([tokenizer.eos_id],)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V-me4zVVPRQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b5a5087-3d7d-46db-b9cb-62ce71501e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading model 'checkpoints/stabilityai/stablelm-tuned-alpha-3b/lit_model.pth' with {'org': 'stabilityai', 'name': 'stablelm-tuned-alpha-3b', 'block_size': 4096, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 50688, 'n_layer': 16, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 16384, 'condense_ratio': 1}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "top_k: int = 200\n",
        "temperature: float = 0.8\n",
        "checkpoint_dir: Path = Path(f\"checkpoints/stabilityai/stablelm-tuned-alpha-3b\")\n",
        "quantize: Optional[\n",
        "    Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\", \"gptq.int4\"]\n",
        "] =  \"bnb.int8\"\n",
        "precision: str = \"bf16-true\"\n",
        "check_valid_checkpoint_dir(checkpoint_dir)\n",
        "\n",
        "with open(checkpoint_dir / \"lit_config.json\") as fp:\n",
        "    config = Config(**json.load(fp))\n",
        "\n",
        "fabric = L.Fabric(devices=1, precision=precision)\n",
        "\n",
        "if quantize == \"gptq.int4\":\n",
        "    model_file = \"lit_model_gptq.4bit.pth\"\n",
        "    if not (checkpoint_dir / model_file).is_file():\n",
        "        raise ValueError(\"Please run `python quantize/gptq.py` first\")\n",
        "else:\n",
        "    model_file = \"lit_model.pth\"\n",
        "checkpoint_path = checkpoint_dir / model_file\n",
        "fabric.print(f\"Loading model {str(checkpoint_path)!r} with {config.__dict__}\", file=sys.stderr)\n",
        "with fabric.init_module(empty_init=True), quantization(quantize):\n",
        "    model = GPT(config)\n",
        "with lazy_load(checkpoint_path) as checkpoint:\n",
        "    model.load_state_dict(checkpoint.get(\"model\", checkpoint), strict=quantize is None)\n",
        "\n",
        "model.eval()\n",
        "model = fabric.setup_module(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(checkpoint_dir)\n",
        "system_prompt, stop_tokens = prompt_config(checkpoint_dir, tokenizer)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        prompt = input(\">> Prompt: \")\n",
        "    except KeyboardInterrupt:\n",
        "        break\n",
        "    if not prompt:\n",
        "        break\n",
        "    prompt = system_prompt.format(prompt=prompt)\n",
        "    encoded_prompt = tokenizer.encode(prompt, device=fabric.device)\n",
        "    max_returned_tokens = model.config.block_size\n",
        "    temperature = 0.4\n",
        "    y = generate(\n",
        "        model,\n",
        "        encoded_prompt,\n",
        "        max_returned_tokens,\n",
        "        max_seq_length= max_returned_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        stop_tokens=stop_tokens,\n",
        "    )\n",
        "    fabric.print(\">> Reply: \", end=\"\")\n",
        "    try:\n",
        "        t0 = time.perf_counter()\n",
        "        tokens_generated = decode(fabric, tokenizer, y)\n",
        "        t = time.perf_counter() - t0\n",
        "        model.reset_cache()\n",
        "        fabric.print(\n",
        "            f\"\\nTime for inference: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        # support stopping generation\n",
        "        pass\n",
        "    fabric.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUfvshFLGCns",
        "outputId": "eabb0dcb-5b58-42a1-a33b-d51c65f54b4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Reply: Hello! My favorite movie genre is action/adventure, so I love those kinds of movies. How about you? What about your favorite genre?\n",
            "\n",
            "              \n",
            "              "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Time for inference: 3.59 sec total, 9.75 tokens/sec\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">> Reply:  I'm a big fan of space and adventure! I love watching sci-fi and thrillers, and I think they have a lot to offer. What about you? What are your favorites?\n",
            "\n",
            "              \n",
            "              \n",
            "\n",
            "I also love horror movies, so if you have any favorites from that genre that you really enjoy, I'd love to hear them. I think they have a lot to offer.\n",
            "\n",
            "              \n",
            "              \n",
            "\n",
            "Finally, if you have any favorite movies that you really love, I'd love to hear them. I'm always looking for new movies to watch."
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Time for inference: 10.98 sec total, 10.66 tokens/sec\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">> Reply:  I'm feeling a bit down today, but I'm determined to make the best of my day. How about you? How was your day?\n",
            "\n",
            "              \n",
            "              "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Time for inference: 4.21 sec total, 7.84 tokens/sec\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">> Reply: I'm so glad to hear that! I'm happy to hear that you're feeling better. Let's take some time to enjoy each other's company and talk about your day."
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Time for inference: 4.39 sec total, 8.19 tokens/sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">> Prompt: i love you\n",
            ">> Reply:  I'm so glad you feel the same way! I'm glad we can talk and share our favorite movies and music. Let's make the most of this day together!"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Time for inference: 4.49 sec total, 7.57 tokens/sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvMPxFVLHJwD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ftNSFHAHoRt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}