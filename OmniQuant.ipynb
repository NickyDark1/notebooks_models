{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhaCXODeXpW1D4CwlkPFoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/OmniQuant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTNvNuX0dvK9",
        "outputId": "28b3c46e-2716-456c-e080-40199bca3cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AutoGPTQ-bugfix'...\n",
            "remote: Enumerating objects: 3109, done.\u001b[K\n",
            "remote: Counting objects: 100% (575/575), done.\u001b[K\n",
            "remote: Compressing objects: 100% (136/136), done.\u001b[K\n",
            "remote: Total 3109 (delta 475), reused 473 (delta 439), pack-reused 2534\u001b[K\n",
            "Receiving objects: 100% (3109/3109), 7.64 MiB | 16.71 MiB/s, done.\n",
            "Resolving deltas: 100% (2062/2062), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ChenMnZ/AutoGPTQ-bugfix.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AutoGPTQ-bugfix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tmb7QJteISU",
        "outputId": "62e5446c-5999-4db4-c889-2adb45922100"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ-bugfix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install g++"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQMDYJbSfLDk",
        "outputId": "1738efce-d14d-4c60-f5a2-a42524ed5fc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
            "g++ set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gekko"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQZm7hf6gMGT",
        "outputId": "284dc5e6-a833-4827-b28d-091f502bf3c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gekko\n",
            "  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from gekko) (1.23.5)\n",
            "Installing collected packages: gekko\n",
            "Successfully installed gekko-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -v ."
      ],
      "metadata": {
        "id": "Jxo_K4TJeMB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import auto_gptq.nn_modules.qlinear.qlinear_cuda as qlinear_cuda\n"
      ],
      "metadata": {
        "id": "yVJOKTSqeQJs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "A6IexEThfGhD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_in_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "import auto_gptq.nn_modules.qlinear.qlinear_cuda as qlinear_cuda\n",
        "from transformers.models.falcon.modeling_falcon import FalconLinear\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import time\n",
        "\n",
        "def get_named_linears(module):\n",
        "    return {name: m for name, m in module.named_modules() if isinstance(m, FalconLinear)}\n",
        "\n",
        "def set_op_by_name(layer, name, new_module):\n",
        "    levels = name.split('.')\n",
        "    if len(levels) > 1:\n",
        "        mod_ = layer\n",
        "        for l_idx in range(len(levels)-1):\n",
        "            if levels[l_idx].isdigit():\n",
        "                mod_ = mod_[int(levels[l_idx])]\n",
        "            else:\n",
        "                mod_ = getattr(mod_, levels[l_idx])\n",
        "        setattr(mod_, levels[-1], new_module)\n",
        "    else:\n",
        "        setattr(layer, name, new_module)"
      ],
      "metadata": {
        "id": "iS3kooRcnTqe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pre_quantized_models/\n"
      ],
      "metadata": {
        "id": "2OMV8Jr2nX5T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/ChenMnZ/falcon-7b-omniquant-w3a16g64 ./pre_quantized_models/falcon-7b-omniquant-w3a16g64\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-51tfy0ncMG",
        "outputId": "768e5d86-87bd-4940-eb2e-941c8ea14df1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into './pre_quantized_models/falcon-7b-omniquant-w3a16g64'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Total 11 (delta 0), reused 0 (delta 0), pack-reused 11\u001b[K\n",
            "Unpacking objects: 100% (11/11), 775.56 KiB | 3.31 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = './pre_quantized_models/falcon-7b-omniquant-w3a16g64'\n",
        "wbits = 3\n",
        "group_size = 64\n",
        "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
        "enc = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForCausalLM.from_config(config=config,torch_dtype=torch.float16, trust_remote_code=True)\n",
        "\n",
        "layers = model.transformer.h\n",
        "for i in tqdm(range(len(layers))):\n",
        "    layer = layers[i]\n",
        "    named_linears = get_named_linears(layer)\n",
        "    for name, module in named_linears.items():\n",
        "        q_linear = qlinear_cuda.QuantLinear(wbits, group_size, module.in_features,module.out_features,not module.bias is None,kernel_switch_threshold=128)\n",
        "        q_linear.to(next(layer.parameters()).device)\n",
        "        set_op_by_name(layer, name, q_linear)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "model.tie_weights()\n",
        "device_map = infer_auto_device_map(model)\n",
        "print(\"Loading pre-computed quantized weights...\")\n",
        "load_checkpoint_in_model(model,checkpoint=model_path,device_map=device_map,offload_state_dict=True)\n",
        "print(\"Loading pre-computed quantized weights Successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZQfZeADnvve",
        "outputId": "92e4a5ce-c9de-4205-f12d-5e6513b5208b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 45.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-computed quantized weights...\n",
            "Loading pre-computed quantized weights Successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "prompt = \"Once upon a time there was a\"\n",
        "input_ids = enc(prompt, return_tensors='pt').input_ids.cuda()\n",
        "model = model.cuda()\n",
        "start_time = time.time()\n",
        "output = model.generate(inputs=input_ids, do_sample=True, top_k=10, max_new_tokens=128)\n",
        "end_time = time.time()\n",
        "speed = len(output[0])/(end_time-start_time)\n",
        "print(enc.decode(output[0]))\n",
        "print(f\"speed:{speed}token/s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVJq0GhOn-9u",
        "outputId": "c685c8b5-ba5b-4758-a971-0e6593102608"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there was a girl who wanted to have a party for her birthday. She was so excited that she couldn't stop talking about it. Every time the topic came up she was so excited. Her birthday finally came and everyone came to celebrate with her but she wasn't really happy at her birthday. She was so sad and upset that no one understood that she wanted her party.\n",
            "The girl was me.\n",
            "When I started to grow up I always wanted something special when it came to celebrating my birthday. I always wanted it to be the best thing ever, but I didn't know how to have the best birthday.\n",
            "I wanted everyone\n",
            "speed:3.4299865540630656token/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What color is the sky?\"\n",
        "input_ids = enc(prompt, return_tensors='pt').input_ids.cuda()\n",
        "model = model.cuda()\n",
        "start_time = time.time()\n",
        "output = model.generate(inputs=input_ids, do_sample=True, top_k=10, max_new_tokens=128)\n",
        "end_time = time.time()\n",
        "speed = len(output[0])/(end_time-start_time)\n",
        "print(enc.decode(output[0]))\n",
        "print(f\"speed:{speed}token/s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vdrg9VOoL_1",
        "outputId": "d2f5e636-b195-4be4-aa70-abb97e5143d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What color is the sky?\n",
            "It’s the first day of summer! So we went to the beach, and it was a perfect day!\n",
            "We got a beach umbrella and sat right next to the water. We had lunch, ate a lot, and had fun playing in the sand and water. When the tide came in, we got to see lots of sea turtles coming to the shore to lay eggs. They were really interesting to watch because some would get up on their back legs and start scratching. They’d scratch and scratch until they got the eggs out from the sand. We watched them all the way until the tide went back out.\n",
            "We\n",
            "speed:3.435994102067567token/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOWgLHMO53xu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}