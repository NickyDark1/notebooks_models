{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/RAGatouille.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x48REecjJWpO"
      },
      "outputs": [],
      "source": [
        "!pip install ragatouille sqlite-utils -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HIkrw6DMkAG",
        "outputId": "4e9970a5-fde9-46b1-9fa1-7f6c86d1d654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping faiss-cpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall --y faiss-cpu & pip install faiss-gpu -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JYwRIudEJaxW"
      },
      "outputs": [],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "import re\n",
        "import sqlite_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lgA_-xLgKCvv"
      },
      "outputs": [],
      "source": [
        "_tags_re = re.compile(r'<[^>]+>')\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    return _tags_re.sub('', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFt0Z9w6Lq3-",
        "outputId": "d2c937c9-9732-45a5-ced8-a382f9b29605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-29 12:27:57--  https://datasette.simonwillison.net/simonwillisonblog.db\n",
            "Resolving datasette.simonwillison.net (datasette.simonwillison.net)... 173.194.192.121, 2607:f8b0:4001:c62::79\n",
            "Connecting to datasette.simonwillison.net (datasette.simonwillison.net)|173.194.192.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘simonwillisonblog.db’\n",
            "\n",
            "simonwillisonblog.d     [        <=>         ]  83.35M  16.4MB/s    in 5.2s    \n",
            "\n",
            "2024-01-29 12:28:02 (16.1 MB/s) - ‘simonwillisonblog.db’ saved [87400448]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://datasette.simonwillison.net/simonwillisonblog.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Db9MN1_YML7N"
      },
      "outputs": [],
      "source": [
        "def go():\n",
        "    db = sqlite_utils.Database(\"simonwillisonblog.db\")\n",
        "    rag = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    entries = list(db[\"blog_entry\"].rows)\n",
        "    entry_texts = [\n",
        "        entry[\"title\"] + '\\n' + strip_html_tags(entry[\"body\"])\n",
        "        for entry in entries\n",
        "    ]\n",
        "    print(\"len of entry_texts is\", len(entry_texts))\n",
        "    entry_ids = [str(entry[\"id\"]) for entry in entries]\n",
        "    entry_metadatas = [\n",
        "        {\"slug\": entry[\"slug\"], \"created\": entry[\"created\"]} for entry in entries\n",
        "    ]\n",
        "    rag.index(\n",
        "        collection=entry_texts,\n",
        "        document_ids=entry_ids,\n",
        "        document_metadatas=entry_metadatas,\n",
        "        index_name=\"blog\",\n",
        "        max_document_length=180,\n",
        "        split_documents=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "aXn-NX_aMYIt",
        "outputId": "124ce96f-e4bb-4fe3-82e6-51800abed4da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len of entry_texts is 3049\n",
            "________________________________________________________________________________\n",
            "WARNING! You have a GPU available, but only `faiss-cpu` is currently installed.\n",
            " This means that indexing will be slow. To make use of your GPU.\n",
            "Please install `faiss-gpu` by running:\n",
            "pip uninstall --y faiss-cpu & pip install faiss-gpu\n",
            " ________________________________________________________________________________\n",
            "Will continue with CPU indexing in 5 seconds...\n",
            "\n",
            "\n",
            "[Jan 29, 12:32:43] #> Note: Output directory .ragatouille/colbert/indexes/blog already exists\n",
            "\n",
            "\n",
            "[Jan 29, 12:32:43] #> Will delete 1 files already at .ragatouille/colbert/indexes/blog in 20 seconds...\n",
            "[Jan 29, 12:33:04] [0] \t\t #> Encoding 9785 passages..\n",
            "[Jan 29, 12:33:38] [0] \t\t avg_doclen_est = 121.8592758178711 \t len(local_sample) = 9,785\n",
            "[Jan 29, 12:33:38] [0] \t\t Creating 16,384 partitions.\n",
            "[Jan 29, 12:33:38] [0] \t\t *Estimated* 1,192,393 embeddings.\n",
            "[Jan 29, 12:33:38] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/blog/plan.json ..\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e219d4ec9c81>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-06aee17d7b49>\u001b[0m in \u001b[0;36mgo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"slug\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"slug\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[0;32m---> 14\u001b[0;31m     rag.index(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcollection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentry_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdocument_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentry_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragatouille/RAGPretrainedModel.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, collection, document_ids, document_metadatas, index_name, overwrite_index, max_document_length, split_documents, document_splitter_fn, preprocessing_fn)\u001b[0m\n\u001b[1;32m    181\u001b[0m         }\n\u001b[1;32m    182\u001b[0m         \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection_with_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         return self.model.index(\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mpid_docid_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpid_docid_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragatouille/models/colbert.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, collection, pid_docid_map, docid_metadata_map, index_name, max_document_length, overwrite)\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavoid_fork_if_possible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         self.indexer.index(\n\u001b[0m\u001b[1;32m    350\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/indexer.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, name, collection, overwrite)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex_does_not_exist\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'reuse'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/indexer.py\u001b[0m in \u001b[0;36m__launch\u001b[0;34m(self, collection)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mshared_queues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mshared_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_without_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_queues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/infra/launcher.py\u001b[0m in \u001b[0;36mlaunch_without_fork\u001b[0;34m(self, custom_config, *args)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mnew_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_existing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mreturn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_process_without_mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallee\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/infra/launcher.py\u001b[0m in \u001b[0;36mrun_process_without_mp\u001b[0;34m(callee, config, *args)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minherit_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mreturn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/indexing/collection_indexer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(config, collection, shared_lists, shared_queues, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_queues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCollectionIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/indexing/collection_indexer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, shared_lists)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_load_codec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_lists\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Trains centroids from selected passages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mprint_memory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'RANK:{self.rank}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/indexing/collection_indexer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, shared_lists)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheldout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concatenate_and_split_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mprint_memory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'RANK:{self.rank}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/indexing/collection_indexer.py\u001b[0m in \u001b[0;36m_train_kmeans\u001b[0;34m(self, sample, shared_lists)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0margs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_faiss_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/colbert/indexing/collection_indexer.py\u001b[0m in \u001b[0;36mcompute_faiss_kmeans\u001b[0;34m(dim, num_partitions, kmeans_niters, shared_lists, return_value_queue)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/faiss/extra_wrappers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, weights, init_centroids)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/faiss/class_wrappers.py\u001b[0m in \u001b[0;36mreplacement_train\u001b[0;34m(self, x, index, weights)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n, x, index, x_weights)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "go()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e53IGzLaMZLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1326d889-37a5-46a9-8543-ca07c63c89c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading searcher for index blog for the first time... This may take a few seconds\n",
            "[Jan 29, 12:53:34] #> Loading codec...\n",
            "[Jan 29, 12:53:34] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jan 29, 12:53:34] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Jan 29, 12:53:34] #> Loading IVF...\n",
            "[Jan 29, 12:53:34] #> Loading doclens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1380.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Jan 29, 12:53:34] #> Loading codes and residuals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00,  8.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searcher loaded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . what is shot scraper?, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2003,  2915, 26988,  2099,  1029,   102,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103], device='cuda:0')\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "rag = RAGPretrainedModel.from_index(\".ragatouille/colbert/indexes/blog/\")\n",
        "docs = rag.search(\"what is shot scraper?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QARhJCNFSAez",
        "outputId": "de72d72a-1b30-4a4a-c41f-5ed7db69cc79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': \"Running this:\\r\\n./blog-answer.sh 'What is shot-scraper?'\\r\\nOutputs this:\\r\\n\\r\\nShot-scraper is a Python utility that wraps Playwright, providing both a command line interface and a YAML-driven configuration flow for automating the process of taking screenshots of web pages and scraping data from them using JavaScript. It can be used to take one-off screenshots or take multiple screenshots in a repeatable way by defining them in a YAML file. Additionally, it can be used to execute JavaScript on a page and return the resulting value.\\r\\n\\r\\nThat's a really good description of my shot-scraper tool. I checked and none of that output is an exact match to content I had previously published on my blog.\\r\\nQ&amp;A\\r\\nMy talk ended with a Q&amp;A session. Here are the summarized questions and answers.\\r\\n\\r\\n\\r\\nHow does LangChain fit into this?\",\n",
              "  'score': 27.515625,\n",
              "  'rank': 1,\n",
              "  'document_id': '8296',\n",
              "  'document_metadata': {'slug': 'embeddings',\n",
              "   'created': '2023-10-23T13:36:21+00:00'}},\n",
              " {'content': 'I can ask questions like \"what is shot-scraper?\" - it\\'s a piece of software I wrote. And the model kicks back a really good response explaining what it is.\\r\\nNone of the words in that response are words that I wrote on my blog - it\\'s actually a better description than I\\'ve come up myself.\\r\\n\\r\\nShot-scraper is a Python utility that wraps Playwright, providing both a command line interface and a YAML-driven configuration flow for automating the process of taking screenshots of web pages, and for scraping data from them using JavaScript.\\r\\n\\r\\nThis works by running a search for articles relating to that question, gluing them together and sticking the question at the end. That\\'s it. That\\'s the trick.\\r\\nI said it\\'s easy: it\\'s super easy to get an initial demo of this working.',\n",
              "  'score': 27.140625,\n",
              "  'rank': 2,\n",
              "  'document_id': '8285',\n",
              "  'document_metadata': {'slug': 'wordcamp-llms',\n",
              "   'created': '2023-08-27T14:35:07+00:00'}},\n",
              " {'content': \"First, some demos\\r\\nI've implemented this pattern against my blog - 3,000 posts going back over twenty years. I built it using some custom SQLite SQL functions that call the OpenAI APIs.\\r\\nFirst, some examples of questions and answers I ran against my blog:\\r\\nWhat does shot scraper do?\\r\\n\\r\\nShot-scraper is a command-line tool that helps automate the process of taking screenshots of web pages and scraping data from them using JavaScript. It can be used to take one-off screenshots or take multiple screenshots in a repeatable way by defining them in a YAML file. It can also be used to execute JavaScript on a page and return the resulting value.\\r\\n\\r\\nshot-scraper is a tool I released in 2022 - after the 2021 GPT-3 training cut-off date.\",\n",
              "  'score': 26.78125,\n",
              "  'rank': 3,\n",
              "  'document_id': '8222',\n",
              "  'document_metadata': {'slug': 'semantic-search-answers',\n",
              "   'created': '2023-01-13T23:47:30+00:00'}},\n",
              " {'content': \"shot-scraper 0.10 and 0.11\\r\\nshot-scraper is my tool for taking automated screenshots of web pages, built on top of Playwright.\\r\\nBen Welsh has been a key early adopter of shot-scraper, using it to power his news-homepages project which takes screenshots of various news websites and then both tweets the results and uploads them to the News Homepages collection on the Internet Archive.\\r\\nshot-scraper 0.10 is mostly Ben's work: he contributed both a --timeout option and a --browser option to let you install and use browsers other than the Chromium default!\\r\\n(Ben needed this because some news homepages were embedding videos in a format that wasn't supported by Chromium but did work fine in regular Chrome.)\",\n",
              "  'score': 26.140625,\n",
              "  'rank': 4,\n",
              "  'document_id': '8159',\n",
              "  'document_metadata': {'slug': 'weeknotes',\n",
              "   'created': '2022-04-08T20:26:36+00:00'}},\n",
              " {'content': \"It's very easy for feature screenshots in documentation for a web application to drift out-of-date with the latest design of the software itself.\\r\\nshot-scraper is a command-line tool that aims to solve this.\\r\\nYou can use it to take one-off screenshots like this:\\r\\nshot-scraper https://latest.datasette.io/ --height 800\\r\\n\\r\\nOr you can define multiple screenshots in a single YAML file - let's call this shots.yml:\\r\\n- url: https://latest.datasette.io/\\r\\n  height: 800\\r\\n  output: index.png\\r\\n- url: https://latest.datasette.io/fixtures\\r\\n  height: 800\\r\\n  output: database.png\\r\\nAnd run them all at once like this:\\r\\nshot-scraper multi shots.yml\\r\\n\\r\\nThis morning I used shot-scraper to replace all of the existing screenshots in the Datasette documentation with up-to-date, automated equivalents.\",\n",
              "  'score': 25.90625,\n",
              "  'rank': 5,\n",
              "  'document_id': '8199',\n",
              "  'document_metadata': {'slug': 'automating-screenshots',\n",
              "   'created': '2022-10-14T23:44:03+00:00'}},\n",
              " {'content': \"shot-scraper --log-requests\\r\\nshot-scraper is my tool for automating screenshots, built on top of Playwright.\\r\\nIts latest feature was inspired by Datasette Lite.\\r\\nI have an ongoing ambition to get Datasette Lite to work entirely offline, using Service Workers.\\r\\nThe first step is to get it to work without loading external resources - it currently hits PyPI and a separate CDN multiple times to download wheels every time you load the application.\\r\\nTo do that, I need a reliable list of all of the assets that it's fetching.\\r\\nWouldn't it be handy If I could run a command and get a list of those resources?\",\n",
              "  'score': 25.6875,\n",
              "  'rank': 6,\n",
              "  'document_id': '8190',\n",
              "  'document_metadata': {'slug': 'weeknotes',\n",
              "   'created': '2022-09-16T02:55:03+00:00'}},\n",
              " {'content': 'shot-scraper: automated screenshots for documentation, built on Playwright\\nshot-scaper is a new tool that I’ve built to help automate the process of keeping screenshots up-to-date in my documentation. It also doubles as a scraping tool - hence the name - which I picked as a complement to my git scraping and help scraping techniques.\\r\\nUpdate 13th March 2022: The new shot-scraper javascript command can now be used to scrape web pages from the command line.\\r\\nUpdate 14th October 2022: Automating screenshots for the Datasette documentation using shot-scraper offers a tutorial introduction to using the tool.\\r\\nThe problem\\r\\nI like to include screenshots in documentation. I recently started writing end-user tutorials for Datasette, which are particularly image heavy (for example).\\r\\nAs software changes over time, screenshots get out-of-date.',\n",
              "  'score': 25.671875,\n",
              "  'rank': 7,\n",
              "  'document_id': '8151',\n",
              "  'document_metadata': {'slug': 'shot-scraper',\n",
              "   'created': '2022-03-10T00:13:30+00:00'}},\n",
              " {'content': \"Instantly create a GitHub repository to take screenshots of a web page\\nI just released shot-scraper-template, a GitHub repository template that helps you start taking automated screenshots of a web page by filling out a form.\\r\\nshot-scraper is my command line tool for taking screenshots of web pages and scraping data from them using JavaScript.\\r\\nOne of its uses is to help create and maintain screenshots for documentation, making it easy to update them to include changes to the design of the underlying pages.\\r\\nTo make this as easy as possible, I've created a GitHub repository template that automates the process of setting up shot-scraper to run against a URL.\\r\\nTo try it out, start here:\\r\\nhttps://github.com/simonw/shot-scraper-template/generate\\r\\n\\r\\nPick a name for your new repository and paste the URL of the page you want to screenshot into the description field.\",\n",
              "  'score': 25.484375,\n",
              "  'rank': 8,\n",
              "  'document_id': '8154',\n",
              "  'document_metadata': {'slug': 'shot-scraper-template',\n",
              "   'created': '2022-03-14T16:52:27+00:00'}},\n",
              " {'content': \"It can also be used to execute JavaScript on a page and return the resulting value.\\r\\n\\r\\nshot-scraper is a tool I released in 2022 - after the 2021 GPT-3 training cut-off date. This is a very good summary - better I think than anything I've written about shot-scraper myself.\\r\\nWhat museums has Simon been to?\\r\\n\\r\\nSimon has been to the Pitt Rivers museum, the British Museum, the Science Museum, the Natural History Museum, the V&amp;A, the Victoria and Albert Museum, and the Pioneer Museum in Paso Robles. He has also seen the Giant Squid at the Natural History Museum and Charles Darwin's collection at the spirit building.\\r\\n\\r\\nThis is entirely correct, though I've talked about a lot more museums than that on my blog.\\r\\nWhat are some unconventional things you can do with GitHub Actions?\",\n",
              "  'score': 25.40625,\n",
              "  'rank': 9,\n",
              "  'document_id': '8222',\n",
              "  'document_metadata': {'slug': 'semantic-search-answers',\n",
              "   'created': '2023-01-13T23:47:30+00:00'}},\n",
              " {'content': \"Automating screenshots for the Datasette documentation using shot-scraper\\nI released shot-scraper back in March as a tool for keeping screenshots in documentation up-to-date.\\r\\nIt's very easy for feature screenshots in documentation for a web application to drift out-of-date with the latest design of the software itself.\\r\\nshot-scraper is a command-line tool that aims to solve this.\",\n",
              "  'score': 25.203125,\n",
              "  'rank': 10,\n",
              "  'document_id': '8199',\n",
              "  'document_metadata': {'slug': 'automating-screenshots',\n",
              "   'created': '2022-10-14T23:44:03+00:00'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llm -q"
      ],
      "metadata": {
        "id": "HAv8Xz08SGiZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llm keys set openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffqF-HKKSOdw",
        "outputId": "73de3410-5ee5-4bbc-947a-ecfbb7335062"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter key: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"/tmp/out.txt\", \"w\").write(\n",
        "    ' '.join([d['content']\n",
        "    for d in rag.search(\"what is shot scraper?\")]\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4JOQVXdSXlO",
        "outputId": "401177b8-b6e9-481f-b200-fc57d8217518"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7329"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /tmp/out.txt | llm -m gpt-4-turbo --system 'what is shot-scraper?'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_h9CJxNSisw",
        "outputId": "96578138-6993-41a1-deb8-3edaed381036"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shot-scraper is a versatile and powerful Python tool designed for both web page screenshot capture and data scraping. It leverages Playwright to interact with web pages, allowing users to automate screenshot captures and execute JavaScript for data extraction. This tool provides a friendly command line interface and supports a YAML-driven configuration for defining multi-step screenshot and data scraping workflows. Developers and content creators can use shot-scraper to ensure their documentation contains up-to-date screenshots or to scrape web data for various applications, making it an invaluable asset in web development and content management projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "docs = colbert.rerank(query='What is Datasette Lite?', documents=[\n",
        "    d['content'] for d in docs\n",
        "], k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqUT1ZrCSj-B",
        "outputId": "f555961d-1b99-4329-b526-ba192ff0db83"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 28.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoQ6RUVeStGS",
        "outputId": "34d2e159-112b-4a95-99c6-7d32f49e2a8b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': \"shot-scraper --log-requests\\r\\nshot-scraper is my tool for automating screenshots, built on top of Playwright.\\r\\nIts latest feature was inspired by Datasette Lite.\\r\\nI have an ongoing ambition to get Datasette Lite to work entirely offline, using Service Workers.\\r\\nThe first step is to get it to work without loading external resources - it currently hits PyPI and a separate CDN multiple times to download wheels every time you load the application.\\r\\nTo do that, I need a reliable list of all of the assets that it's fetching.\\r\\nWouldn't it be handy If I could run a command and get a list of those resources?\",\n",
              "  'score': 21.125,\n",
              "  'rank': 0,\n",
              "  'result_index': 0},\n",
              " {'content': \"Automating screenshots for the Datasette documentation using shot-scraper\\nI released shot-scraper back in March as a tool for keeping screenshots in documentation up-to-date.\\r\\nIt's very easy for feature screenshots in documentation for a web application to drift out-of-date with the latest design of the software itself.\\r\\nshot-scraper is a command-line tool that aims to solve this.\",\n",
              "  'score': 16.765625,\n",
              "  'rank': 1,\n",
              "  'result_index': 1},\n",
              " {'content': \"It's very easy for feature screenshots in documentation for a web application to drift out-of-date with the latest design of the software itself.\\r\\nshot-scraper is a command-line tool that aims to solve this.\\r\\nYou can use it to take one-off screenshots like this:\\r\\nshot-scraper https://latest.datasette.io/ --height 800\\r\\n\\r\\nOr you can define multiple screenshots in a single YAML file - let's call this shots.yml:\\r\\n- url: https://latest.datasette.io/\\r\\n  height: 800\\r\\n  output: index.png\\r\\n- url: https://latest.datasette.io/fixtures\\r\\n  height: 800\\r\\n  output: database.png\\r\\nAnd run them all at once like this:\\r\\nshot-scraper multi shots.yml\\r\\n\\r\\nThis morning I used shot-scraper to replace all of the existing screenshots in the Datasette documentation with up-to-date, automated equivalents.\",\n",
              "  'score': 15.78125,\n",
              "  'rank': 2,\n",
              "  'result_index': 2},\n",
              " {'content': 'shot-scraper: automated screenshots for documentation, built on Playwright\\nshot-scaper is a new tool that I’ve built to help automate the process of keeping screenshots up-to-date in my documentation. It also doubles as a scraping tool - hence the name - which I picked as a complement to my git scraping and help scraping techniques.\\r\\nUpdate 13th March 2022: The new shot-scraper javascript command can now be used to scrape web pages from the command line.\\r\\nUpdate 14th October 2022: Automating screenshots for the Datasette documentation using shot-scraper offers a tutorial introduction to using the tool.\\r\\nThe problem\\r\\nI like to include screenshots in documentation. I recently started writing end-user tutorials for Datasette, which are particularly image heavy (for example).\\r\\nAs software changes over time, screenshots get out-of-date.',\n",
              "  'score': 14.5,\n",
              "  'rank': 3,\n",
              "  'result_index': 3},\n",
              " {'content': 'I can ask questions like \"what is shot-scraper?\" - it\\'s a piece of software I wrote. And the model kicks back a really good response explaining what it is.\\r\\nNone of the words in that response are words that I wrote on my blog - it\\'s actually a better description than I\\'ve come up myself.\\r\\n\\r\\nShot-scraper is a Python utility that wraps Playwright, providing both a command line interface and a YAML-driven configuration flow for automating the process of taking screenshots of web pages, and for scraping data from them using JavaScript.\\r\\n\\r\\nThis works by running a search for articles relating to that question, gluing them together and sticking the question at the end. That\\'s it. That\\'s the trick.\\r\\nI said it\\'s easy: it\\'s super easy to get an initial demo of this working.',\n",
              "  'score': 9.2421875,\n",
              "  'rank': 4,\n",
              "  'result_index': 4}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"/tmp/out1.txt\", \"w\").write(\n",
        "    ' '.join([d['content']\n",
        "    for d in docs]\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922J3KksSxR0",
        "outputId": "5cac42da-d7f1-4de0-c3b5-d306f7e677f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3394"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /tmp/out1.txt | llm -m gpt-4-turbo --system 'What is Datasette Lite?'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC_ZM0PTSz18",
        "outputId": "43b3c644-f28b-4edb-e5cc-03b0a4327ccf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasette Lite is a version of Datasette, an open-source tool for exploring and publishing data, that runs entirely in the browser using WebAssembly. Datasette typically requires a server to run, but Datasette Lite aims to make it possible to work with datasets directly in the browser without needing any server-side components. This approach potentially makes Datasette accessible in environments where deploying a server is not possible or practical and could facilitate offline use of the tool via Service Workers. The ambition to make Datasette Lite work entirely offline highlights an effort to increase the tool's versatility and accessibility, making it easier for users to interact with and analyze data from anywhere, even without an internet connection.\n",
            "\n",
            "The reference to Datasette Lite working offline and the desire to avoid loading external resources like packages from PyPI or assets from a CDN is part of an effort to make the application more self-contained and efficient. By reducing reliance on external resources, the application can become faster and more reliable, particularly in situations with limited or no internet connectivity.\n",
            "\n",
            "In the context given, shot-scraper, a tool for automating the capture of screenshots from web pages, is being used to document Datasette (and potentially Datasette Lite), by keeping screenshots in the documentation up to date with the latest version of the software. The ability to automatically generate a list of external resources fetched by Datasette Lite using shot-scraper could facilitate the process of making the application work offline by identifying what resources need to be included or cached by the Service Workers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import llm\n"
      ],
      "metadata": {
        "id": "T7f6eTutS-po"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = llm.get_model(\"gpt-4-turbo\")\n",
        "model.key = ''\n"
      ],
      "metadata": {
        "id": "RdKKur8zTYtv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ' '.join([d['content']\n",
        "    for d in docs])"
      ],
      "metadata": {
        "id": "GDTZUaTrUbJa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = model.prompt(\n",
        "    prompt,\n",
        "    system='What is Datasette Lite?'\n",
        ")"
      ],
      "metadata": {
        "id": "K-cM0ofvTVmc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in response:\n",
        "    print(chunk, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFtdDllRTd7p",
        "outputId": "e08312f3-fa89-4209-9117-91f9d4b8a82b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasette Lite is an experimental version of Datasette, a tool aimed at making it easier for journalists, researchers, and software developers to explore, analyze, and publish large datasets. While the original Datasette requires a server environment to run, Datasette Lite seeks to bring the core functionalities of Datasette to the web browser, leveraging technologies such as WebAssembly and SQLite.\n",
            "\n",
            "The idea behind Datasette Lite is to run entirely in the client's browser, eliminating the need for server-side setup or execution. This makes Datasette more accessible for casual users or those who wish to quickly explore datasets without dealing with the complexities of server configuration or database management.\n",
            "\n",
            "One of the goals for Datasette Lite, as mentioned, is to work entirely offline using Service Workers. This would allow users to access and interact with their data without an internet connection, once the initial load is complete. Achieving this involves ensuring that all necessary resources and data are cached locally in the browser and that the application can function correctly using those resources.\n",
            "\n",
            "The ambition to make Datasette Lite work without loading external resources is aimed at improving performance, reliability, and the user experience, especially in environments with poor or unreliable internet connectivity. It involves bundling all necessary assets and dependencies with the application itself, so everything needed to run is available locally on the user's device.\n",
            "\n",
            "Datasette Lite, with its focus on running entirely within the browser and potentially offline, represents an interesting evolution of data exploration tools. It seeks to make data analysis more accessible and portable, leveraging modern web technologies to bring powerful data exploration capabilities directly to users' browsers, without the overhead associated with traditional server-based applications."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KHcecBcAULtx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcLEKefFAPgbwn2DM+p/Wn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}