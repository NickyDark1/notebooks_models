{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7131dc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-23 12:09:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M   980KB/s    in 1.1s    \n",
      "\n",
      "2023-04-23 12:09:08 (980 KB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f30df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e34655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c596e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79653b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fb49ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(''.join(chars))\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a3b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018b6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc18645",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[x] for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d72ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = lambda l: \"\".join([itos[x] for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0efaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 47, 1, 58, 46, 43, 56, 43]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e22c74c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi there'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([46, 47, 1, 58, 46, 43, 56, 43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e943d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc05388",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f27fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]), torch.Tensor)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f67b1902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66922688",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a23ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9cd909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62038547",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78a76ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfd62ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed9341b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc4bde3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18, 47, 56, 57, 58,  1, 15, 47]),\n",
       " tensor([47, 56, 57, 58,  1, 15, 47, 58]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f98640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctx  tensor([18]) target tensor(47)\n",
      "ctx  tensor([18, 47]) target tensor(56)\n",
      "ctx  tensor([18, 47, 56]) target tensor(57)\n",
      "ctx  tensor([18, 47, 56, 57]) target tensor(58)\n",
      "ctx  tensor([18, 47, 56, 57, 58]) target tensor(1)\n",
      "ctx  tensor([18, 47, 56, 57, 58,  1]) target tensor(15)\n",
      "ctx  tensor([18, 47, 56, 57, 58,  1, 15]) target tensor(47)\n",
      "ctx  tensor([18, 47, 56, 57, 58,  1, 15, 47]) target tensor(58)\n"
     ]
    }
   ],
   "source": [
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"ctx \", context, \"target\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d8ab5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1130ef270>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f400257",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e200b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2621be6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 5, 3, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(6, (4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ef9ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ed4a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f083be50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1]) ----- tensor(58)\n",
      "tensor([ 1, 58]) ----- tensor(46)\n",
      "tensor([ 1, 58, 46]) ----- tensor(43)\n",
      "tensor([ 1, 58, 46, 43]) ----- tensor(1)\n",
      "tensor([ 1, 58, 46, 43,  1]) ----- tensor(46)\n",
      "tensor([ 1, 58, 46, 43,  1, 46]) ----- tensor(53)\n",
      "tensor([ 1, 58, 46, 43,  1, 46, 53]) ----- tensor(54)\n",
      "tensor([ 1, 58, 46, 43,  1, 46, 53, 54]) ----- tensor(43)\n",
      "tensor([53]) ----- tensor(1)\n",
      "tensor([53,  1]) ----- tensor(39)\n",
      "tensor([53,  1, 39]) ----- tensor(1)\n",
      "tensor([53,  1, 39,  1]) ----- tensor(61)\n",
      "tensor([53,  1, 39,  1, 61]) ----- tensor(47)\n",
      "tensor([53,  1, 39,  1, 61, 47]) ----- tensor(44)\n",
      "tensor([53,  1, 39,  1, 61, 47, 44]) ----- tensor(43)\n",
      "tensor([53,  1, 39,  1, 61, 47, 44, 43]) ----- tensor(0)\n",
      "tensor([39]) ----- tensor(52)\n",
      "tensor([39, 52]) ----- tensor(42)\n",
      "tensor([39, 52, 42]) ----- tensor(56)\n",
      "tensor([39, 52, 42, 56]) ----- tensor(63)\n",
      "tensor([39, 52, 42, 56, 63]) ----- tensor(1)\n",
      "tensor([39, 52, 42, 56, 63,  1]) ----- tensor(44)\n",
      "tensor([39, 52, 42, 56, 63,  1, 44]) ----- tensor(53)\n",
      "tensor([39, 52, 42, 56, 63,  1, 44, 53]) ----- tensor(56)\n",
      "tensor([58]) ----- tensor(57)\n",
      "tensor([58, 57]) ----- tensor(1)\n",
      "tensor([58, 57,  1]) ----- tensor(52)\n",
      "tensor([58, 57,  1, 52]) ----- tensor(53)\n",
      "tensor([58, 57,  1, 52, 53]) ----- tensor(40)\n",
      "tensor([58, 57,  1, 52, 53, 40]) ----- tensor(53)\n",
      "tensor([58, 57,  1, 52, 53, 40, 53]) ----- tensor(42)\n",
      "tensor([58, 57,  1, 52, 53, 40, 53, 42]) ----- tensor(63)\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b][:t+1]\n",
    "        target = yb[b][t]\n",
    "        print(context, \"-----\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9da57db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24f19061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_decay_matrix(dim, gamma):\n",
    "    d = torch.ones(dim)\n",
    "    d = torch.tril(d)\n",
    "    \n",
    "    for index, head in enumerate(d):\n",
    "        g = gamma[index]\n",
    "        for idx, x in enumerate(torch.tril(head)):\n",
    "            for idy, y in enumerate(x):\n",
    "                if idx >= idy:\n",
    "                    head[idx][idy] = g ** (idx-idy)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "185a809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from einops import rearrange, reduce, repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2a585da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChunkwiseRetention(nn.Module):\n",
    "    def __init__(self, chunk_size, num_head, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed,  chunk_size * num_head, bias = False)\n",
    "        self.query = nn.Linear(n_embed,  chunk_size * num_head, bias = False)\n",
    "        self.value = nn.Linear(n_embed,  chunk_size * num_head, bias = False)\n",
    "        self.gamma = 1.0-2.0**(-5-torch.arange(0,num_head))\n",
    "        self.decay_mask = get_decay_matrix((num_head, block_size, block_size), self.gamma)\n",
    "        self.chunk_decay = self.gamma\n",
    "        self.gn = nn.GroupNorm(1, num_head)\n",
    "        self.num_head = num_head\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "    def forward(self, x, past_kv):\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        k = rearrange(k, ('b t (h c) -> b h t c'), t=T, h=self.num_head, c =self.chunk_size)\n",
    "        q = rearrange(q, ('b t (h c) -> b h t c'), t=T, h=self.num_head, c =self.chunk_size)\n",
    "        v = rearrange(v, ('b t (h c) -> b h t c'), t=T, h=self.num_head, c =self.chunk_size)\n",
    "        \n",
    "    \n",
    "        retention = q @ k.transpose(-1, -2)\n",
    "        \n",
    "        # b h t c , b h c t -> b h t t \n",
    "    \n",
    "        retention = retention  * self.decay_mask   # b h t t* h t t\n",
    "        \n",
    "        \n",
    "        \n",
    "        inner_retention = retention @ v\n",
    "        \n",
    "        \n",
    "        past_kv = repeat(past_kv, 'n q v -> B n q v', B=B)\n",
    "        pb, pn, pq, pv = past_kv.shape\n",
    "        \n",
    "        padding = torch.zeros(pb, pn, pq, self.chunk_size)\n",
    "        \n",
    "        past_kv = past_kv+ padding\n",
    "        \n",
    "        \n",
    "        \n",
    "        dm = repeat(self.decay_mask, 'h c d -> B h c d', B=B)\n",
    "        pp = q @ past_kv\n",
    "        cross_retention = pp.transpose(-1, -2) @ dm\n",
    "        cross_retention = cross_retention.transpose(-1, -2)\n",
    "        \n",
    "        \n",
    "        retention = inner_retention + cross_retention\n",
    "        \n",
    "        \n",
    "        \n",
    "        current_kv = self.gamma.view(self.num_head, 1, 1) * past_kv + (k.transpose(-1, -2) @ v)\n",
    "        output = self.gn(retention.transpose(-1,-2))\n",
    "        output = rearrange(output, 'b c h t -> b t (c h)')\n",
    "        return output, current_kv.mean(dim=0)\n",
    "    \n",
    "    \n",
    "class GatedMultiScaleRetention(nn.Module):\n",
    "    def __init__(self, chunk_size, num_head, block_size):\n",
    "        super().__init__()\n",
    "        self.wg = nn.Linear(n_embed,  n_embed, bias = False)\n",
    "        self.act = nn.SiLU()\n",
    "        self.y= ChunkwiseRetention(num_head = n_head, chunk_size = n_embed//n_head, block_size=block_size)\n",
    "        self.wo = nn.Linear(n_embed,  n_embed, bias = False)\n",
    "        self.past = torch.zeros(num_head, chunk_size, chunk_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        wgx = self.wg(x)\n",
    "        wgx = self.act(wgx)\n",
    "        y, past = self.y(wgx, self.past)\n",
    "        self.past = past.detach()\n",
    "        y = wgx * y\n",
    "        return self.wo(y)\n",
    "        \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4* n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "         nn.Dropout(dropout))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, block_size):\n",
    "        super().__init__()\n",
    "        self.sa_head= GatedMultiScaleRetention(num_head = n_head, chunk_size = n_embed//n_head, block_size=block_size)\n",
    "        self.ffw=  FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_head(self.ln1(x))\n",
    "        x = x+self.ffw(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "\n",
    "class RetNet(nn.Module):\n",
    "    def __init__(self, block_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, block_size=block_size) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokes):\n",
    "        for _ in range(max_new_tokes):\n",
    "            b, s = idx.shape\n",
    "            bk = min(s, block_size)\n",
    "            idx_cond =  torch.cat((torch.zeros(b, block_size-bk, dtype=int), idx), dim=1)[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35bc917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "n_embed = 32\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9b07fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24122aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetNet(block_size=block_size)\n",
    "xb, yb = get_batch('train', batch_size=batch_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "logits, loss = model(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "88567e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18a31597",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size=batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba7349df",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48c3a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e43b298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5199, val loss 4.5151\n",
      "step 100: train loss 2.8002, val loss 2.8240\n",
      "step 200: train loss 2.6565, val loss 2.7735\n",
      "step 300: train loss 2.6466, val loss 2.8999\n",
      "step 400: train loss 2.8889, val loss 3.2265\n",
      "step 500: train loss 2.4477, val loss 2.6071\n",
      "step 600: train loss 2.5639, val loss 2.8179\n",
      "step 700: train loss 2.3692, val loss 2.6647\n",
      "step 800: train loss 2.9142, val loss 3.4041\n",
      "step 900: train loss 2.2591, val loss 2.3663\n",
      "step 1000: train loss 2.5290, val loss 2.9728\n",
      "step 1100: train loss 2.2890, val loss 2.3944\n",
      "step 1200: train loss 2.7051, val loss 3.1527\n",
      "step 1300: train loss 2.5241, val loss 2.8266\n",
      "step 1400: train loss 2.3235, val loss 2.5941\n",
      "step 1500: train loss 2.5920, val loss 3.0807\n",
      "step 1600: train loss 2.5082, val loss 2.8587\n",
      "step 1700: train loss 2.6868, val loss 2.9852\n",
      "step 1800: train loss 2.6550, val loss 3.3294\n",
      "step 1900: train loss 3.2855, val loss 3.4901\n",
      "step 2000: train loss 2.6440, val loss 3.1719\n",
      "step 2100: train loss 2.8744, val loss 3.6023\n",
      "step 2200: train loss 2.3715, val loss 2.5404\n",
      "step 2300: train loss 2.7079, val loss 3.0241\n",
      "step 2400: train loss 2.4958, val loss 2.8192\n",
      "step 2500: train loss 2.7234, val loss 3.0621\n",
      "step 2600: train loss 2.2912, val loss 2.5069\n",
      "step 2700: train loss 2.1890, val loss 2.5421\n",
      "step 2800: train loss 2.6313, val loss 3.4413\n",
      "step 2900: train loss 2.1856, val loss 2.4894\n",
      "step 3000: train loss 2.1295, val loss 2.4997\n",
      "step 3100: train loss 2.2946, val loss 2.8144\n",
      "step 3200: train loss 2.4222, val loss 2.9552\n",
      "step 3300: train loss 2.8753, val loss 3.6455\n",
      "step 3400: train loss 3.1708, val loss 3.8959\n",
      "step 3500: train loss 2.2445, val loss 2.6516\n",
      "step 3600: train loss 2.1229, val loss 2.4257\n",
      "step 3700: train loss 2.3356, val loss 2.8208\n",
      "step 3800: train loss 2.6160, val loss 3.2764\n",
      "step 3900: train loss 2.3190, val loss 2.7470\n",
      "step 4000: train loss 1.9961, val loss 2.5904\n",
      "step 4100: train loss 2.0443, val loss 2.5646\n",
      "step 4200: train loss 1.9736, val loss 2.3770\n",
      "step 4300: train loss 2.6103, val loss 3.8953\n",
      "step 4400: train loss 2.1000, val loss 2.7155\n",
      "step 4500: train loss 1.8610, val loss 2.1778\n",
      "step 4600: train loss 1.8771, val loss 2.5057\n",
      "step 4700: train loss 1.9368, val loss 2.3528\n",
      "step 4800: train loss 1.8832, val loss 2.4036\n",
      "step 4900: train loss 2.0834, val loss 2.5883\n",
      "step 4999: train loss 2.0971, val loss 2.8457\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % 100 == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size=batch_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0802d00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HUThioth, faledisw brohid hais anow gioun'nthaseriloay mo; thavehen LAwar bobul, traga mironthaltith, oul tha be mtou; so.\n",
      "SThald CORThen ns hwid boucoche f gwitifen l?\n",
      "I titin ild itsed p geanovAnh s\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79b299ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I tacoghedinofo fstridil iraing o, tmitio itsoshene f:\n",
      "Astetne hai on o by ardin urdit Ci sp teate d d oigdost s HAnont?\n",
      "ICantod homs coke ayer ENZtousu.\n",
      "In'Thitaidorncosy f venely nou-QY s dEy.\n",
      "O:\n",
      "SC\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c017178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b3b101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "940b48b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15339, 1917]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e310b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e7882a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301829"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "080169bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad84ce6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([text_tokens[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55ff9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(text_tokens, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48f7d4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([301829])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb0fd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "49c6d83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169893"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "177f3d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69c21edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text.split(' '))))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4871f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWas'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8723537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \" \".join([itos[x] for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "27375acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text.split(' ')), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f04ec236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1455,   957, 39874, 29614,  5949, 16628, 18572, 24432, 34050, 34057])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7ef6e940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(text.split(' ')[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7601b1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First',\n",
       " 'Citizen:\\nBefore',\n",
       " 'we',\n",
       " 'proceed',\n",
       " 'any',\n",
       " 'further,',\n",
       " 'hear',\n",
       " 'me',\n",
       " 'speak.\\n\\nAll:\\nSpeak,',\n",
       " 'speak.\\n\\nFirst']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(' ')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb845c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a4cd38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "n_embed = 32\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a0133e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetNet(block_size=block_size)\n",
    "xb, yb = get_batch('train', batch_size=batch_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "logits, loss = model(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9fecca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 7.2402, val loss 9.8991\n",
      "step 100: train loss 6.9271, val loss 10.1411\n",
      "step 200: train loss 6.2498, val loss 9.3603\n",
      "step 300: train loss 6.1858, val loss 9.5330\n",
      "step 400: train loss 6.1536, val loss 9.6789\n",
      "step 500: train loss 6.0791, val loss 9.7732\n",
      "step 600: train loss 6.9841, val loss 10.4825\n",
      "step 700: train loss 6.0675, val loss 9.9038\n",
      "step 800: train loss 6.2187, val loss 10.2573\n",
      "step 900: train loss 6.2173, val loss 10.2595\n",
      "step 1000: train loss 5.8269, val loss 10.1152\n",
      "step 1100: train loss 6.1537, val loss 10.7570\n",
      "step 1200: train loss 5.7523, val loss 10.2635\n",
      "step 1300: train loss 6.0812, val loss 10.6564\n",
      "step 1400: train loss 5.6627, val loss 10.4134\n",
      "step 1500: train loss 5.7125, val loss 10.4211\n",
      "step 1600: train loss 5.4483, val loss 10.4002\n",
      "step 1700: train loss 6.1413, val loss 11.2838\n",
      "step 1800: train loss 5.9623, val loss 11.1319\n",
      "step 1900: train loss 5.5093, val loss 10.5941\n",
      "step 2000: train loss 5.3935, val loss 10.6224\n",
      "step 2100: train loss 5.5008, val loss 10.9941\n",
      "step 2200: train loss 5.6656, val loss 11.1429\n",
      "step 2300: train loss 5.4165, val loss 11.0075\n",
      "step 2400: train loss 5.6308, val loss 11.1456\n",
      "step 2500: train loss 5.6122, val loss 11.3748\n",
      "step 2600: train loss 5.2973, val loss 11.0839\n",
      "step 2700: train loss 7.1702, val loss 13.3893\n",
      "step 2800: train loss 5.4979, val loss 11.0774\n",
      "step 2900: train loss 5.4640, val loss 11.2021\n",
      "step 3000: train loss 5.6821, val loss 11.4878\n",
      "step 3100: train loss 5.2798, val loss 11.5744\n",
      "step 3200: train loss 5.6312, val loss 12.2176\n",
      "step 3300: train loss 6.5307, val loss 12.2462\n",
      "step 3400: train loss 5.1570, val loss 11.3373\n",
      "step 3500: train loss 5.2610, val loss 11.6277\n",
      "step 3600: train loss 5.9500, val loss 12.4251\n",
      "step 3700: train loss 5.1417, val loss 11.5137\n",
      "step 3800: train loss 5.3325, val loss 11.9058\n",
      "step 3900: train loss 4.9464, val loss 11.4826\n",
      "step 4000: train loss 5.1929, val loss 11.9806\n",
      "step 4100: train loss 5.2831, val loss 12.2481\n",
      "step 4200: train loss 5.2404, val loss 12.0363\n",
      "step 4300: train loss 5.1159, val loss 12.1123\n",
      "step 4400: train loss 4.8665, val loss 11.7406\n",
      "step 4500: train loss 5.0160, val loss 12.1974\n",
      "step 4600: train loss 5.4214, val loss 12.2131\n",
      "step 4700: train loss 5.0268, val loss 11.9813\n",
      "step 4800: train loss 4.9407, val loss 11.8241\n",
      "step 4900: train loss 6.0261, val loss 12.9589\n",
      "step 4999: train loss 5.3374, val loss 12.1293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % 100 == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size=batch_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "58b6580e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou art kneel before king this holy friar, now, triumph like to the flood?\n",
      "Yet bands before what we cheque to their words and now the apostle vengeance to the market-place!\n",
      "You will would\n",
      "When husband lurch'd a father from the comfort so dishonour'd me hear this business straight;\n",
      "Go, more fair time you'll make, as e'er that fear'd where suspicion from my lord, I'll buy justice, justice!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Relate hopeless restitution, I'll depose thrice fond heaven\n",
      "Boldly delight to the most kindred unto Bianca\n",
      "Till Katharina blame if the moiety. thou revenged the contrary gives young Rutland, prosecute\n",
      "'Gainst quench his ears, for what's a case of you.\n",
      "\n",
      "ISABELLA:\n",
      "Good Come, the lords who stood Susan before.\n",
      "\n",
      "KING as an\n",
      "inventory and one friend.\n",
      "\n",
      "KING man of my day,\n",
      "That service for the fume O, scars and, of him;\n",
      "Bear my body's length.\n",
      "Why, 'tis charity.\n",
      "What, return.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Nay, for my remission copy of the greatest her unto mine own noble lord.\n",
      "\n",
      "BRAKENBURY:\n",
      "Why expected.\n",
      "\n",
      "ISABELLA:\n",
      "Ho, without him.\n",
      "\n",
      "AEdile:\n",
      "Worthy back, and makes you know not\n",
      "how by the nurse in set honourable them: Jupiter\n",
      "Became in probation live, make force but riddling and rivals ill;\n",
      "Ill issue,\n",
      "Against the haste you collected trudge about\n",
      "Through Claudio, think it will render it should be so harsh, fair gentlewoman.\n",
      "\n",
      "Nurse:\n",
      "Is me to pieces since you please, no blemish almost hangmen. with how respect,\n",
      "Show\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([encode(\"thou art kneel before king\".split(' '))], dtype = torch.long)\n",
    "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6244a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou art kneel before king he that her no, such delicate\n",
      "burthens shall stay with rice? man said that hast as wise For Come, Escalus,\n",
      "You will I.\n",
      "\n",
      "DUCHESS i' the king till my upon! you'll continue so bad lord, you in twain.\n",
      "\n",
      "CLAUDIO:\n",
      "But that's mine own noble ladies,--and all to me!\n",
      "God for them than a pretty Camillo,--\n",
      "But friar, denies prudence; city gates,\n",
      "Speak at bowls.\n",
      "\n",
      "QUEEN:\n",
      "'Twill had woe! with him.\n",
      "Off curses may live:\n",
      "There alack the world wise say'st thou hast thou sworest that drops of their idle body,\n",
      "But to unwise you\n",
      "To will serve\n",
      "For you in the husband or is my peril in virtue,\n",
      "I as your wisdom be\n",
      "not tainted! even that you some Paris; as I will all acknowledge. Lord Hastings, which I should name of his own power: you to we\n",
      "shall let us as you,\n",
      "A nuns, upon my master time against his summer butterflies,\n",
      "Or butchers 'Romeo who, the spices and say where he hears even drag no healthsome man was been beaten true sign is he? I well.\n",
      "\n",
      "ANGELO:\n",
      "Good father, is, or woe.\n",
      "\n",
      "Nurse:\n",
      "I cheque the Slys father.\n",
      "Unreasonable creatures Lancaster from suspicion!\n",
      "\n",
      "DORSET:\n",
      "A boon, upon my head of yours,\n",
      "So season'd are so?\n",
      "\n",
      "COMINIUS:\n",
      "Marcius,\n",
      "We Henry's corse,\n",
      "When he's gone.\n",
      "\n",
      "Children:\n",
      "What made him, in a jealous God! better Prince for a destroyer sent for it. it for an\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([encode(\"thou art kneel before king\".split(' '))], dtype = torch.long)\n",
    "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "35b54cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hermione in God's enemy,\n",
      "God is having already mine majesty may remain that afford any fear\n",
      "Lesser him brazen pale to the other of Capulet's\n",
      "Sups me with age, ache, cause and not queen upon his treasure is:\n",
      "He do know half you hour mightst have but forth Jesu money, with all slaughter'd lands.\n",
      "\n",
      "LADY GREY:\n",
      "Be reform'd: that is the news?\n",
      "\n",
      "COMINIUS:\n",
      "Your law repair of my lord; our security,\n",
      "Grows wife, that can wit but to the fault of my neighbour youngest promises a bear.\n",
      "\n",
      "MENENIUS:\n",
      "He's thy sort.\n",
      "\n",
      "RICHARD:\n",
      "'Twas be crowned forty years,\n",
      "My wretch,\n",
      "Grace for all my gracious lord:\n",
      "I dream till since thee Caesar never grow.\n",
      "\n",
      "GARDENER:\n",
      "Poor so? so, nor vantage I'll not whom the duke and tie bloody cannibals!\n",
      "How your cousin?\n",
      "\n",
      "JULIET:\n",
      "Shall I am glad RICHARD II:\n",
      "Join your hand,\n",
      "And know you may.\n",
      "Say, down, sure, Tybalt, the worse:\n",
      "Fell love, many shepherd is importune his rest,\n",
      "That a day and let us here longer.\n",
      "\n",
      "LEONTES:\n",
      "Ha!\n",
      "\n",
      "CAMILLO:\n",
      "Stays toil, the lieutenant comes.\n",
      "Master bitterly:\n",
      "'Yea,' valiant or wrong,\n",
      "Lord it concerns for the scandal already tiger's ignorant a\n",
      "pedlar, to Romeo?\n",
      "\n",
      "FRIAR JOHN:\n",
      "I I saw valiant Roman,\n",
      "These no, my good nurse, by this.\n",
      "Therefore, a senate\n",
      "The cockle that my lord,\n",
      "That thus descended,\n",
      "That away; my BOLINGBROKE:\n",
      "My dust of your very trick\n",
      "For my lusts\n",
      "Burn his; you, on, arms, with any colour.\n",
      "\n",
      "COMINIUS:\n",
      "Nay, are fond,\n",
      "Lascivious metres, mercy thou\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([encode(\"Hermione\".split(' '))], dtype = torch.long)\n",
    "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4396af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come mind my son of my country's comfort on an ounce.\n",
      "\n",
      "AUTOLYCUS:\n",
      "\n",
      "Shepherd:\n",
      "Well, with fire\n",
      "Our ever your great guess, as easy\n",
      "As sin!\n",
      "Shall 'tis an unpitied end:\n",
      "Earth waged her suit;\n",
      "It whose tongue beat thee here. hands: my fierce good, his princely semblance\n",
      "Are of my head to me,\n",
      "And nor your way.\n",
      "\n",
      "VOLUMNIA:\n",
      "O, man;\n",
      "With his discords at the noon-tide night.\n",
      "Princes be loud am a thief to avoid,\n",
      "And by blood and wear favour do some MARGARET:\n",
      "Thy woes desperately never man\n",
      "Sigh'd pardon to hear to sweetly any you in pieces toward men, his assistants, as with fantastic not hurt not much amiss: mother should miss off a dangerous words bewitch me tell me,\n",
      "In gentle Percy; as is no other, or mark you\n",
      "His legs and speak any scruple; your gate\n",
      "And therein,\n",
      "And shall advise you\n",
      "To sit She sir, on Warwick, tongue in now.\n",
      "\n",
      "PETER:\n",
      "You cannot lose their dusky graves.\n",
      "Richard fair for some bond, the seat, you both I cannot more than the hire\n",
      "Of 'gainst the dark.\n",
      "\n",
      "MERCUTIO:\n",
      "If wounded forth; painted EDWARD:\n",
      "Nay, days,\n",
      "To long with tears\n",
      "And BOLINGBROKE:\n",
      "I swear.\n",
      "\n",
      "THOMAS MOWBRAY:\n",
      "And seeing ill.\n",
      "Thy death-bed you leave the restful hate that enters next,\n",
      "Thinking with hunt's-up my own,--be satisfied.\n",
      "\n",
      "MERCUTIO:\n",
      "O Who strumpet Shore,\n",
      "That and remain, sit, tribunes you walk myself\n",
      "To Take the house unto his foul traitor's noble duke\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([encode(\"come\".split(' '))], dtype = torch.long)\n",
    "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd64ee6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
