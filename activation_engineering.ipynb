{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMPIbBkwfoGD6zoEauGh/8e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/activation_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N8B-LpN_6ft",
        "outputId": "37b59d1b-ca11-4cb5-c965-7d6b0e182277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=50fba5a572f4a8b12ef1ea4467958c16b6b09376723a4e235b67042dee871b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.33.3\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-33M\").to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n"
      ],
      "metadata": {
        "id": "adETqSbw_9TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time there was\"\n",
        "\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=500, do_sample=True)\n",
        "\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print()\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie8L3Q2iAFF3",
        "outputId": "23152eb6-c1c9-4f7e-bb51-e07770a3d72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Once upon a time there was a little girl named Annie. She had a favorite sock that she always wore. It was the softest, most comfortable sock ever with her all the colors and textures. One day, at night, Annie noticed the sock had vanished from her drawer! She looked everywhere for it, but it wasn't there. Annie was very sad so she asked her mom for help. Her mom suggested she look in the park, maybe they could find it. Together, they searched up and down and in all the things they did. Suddenly, the sock came flying through the air! Annie was so glad, she hugged her mom and thanked her for helping to get the sock back. From that night on, Annie always made sure to keep her sock safe and sound!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def get_activated_model(model, plus_prompt, minus_prompt, plus_coeff, minus_coeff, layer):\n",
        "  activation_model = deepcopy(model)\n",
        "  orig = model.transformer.h[layer].forward\n",
        "\n",
        "  activation = plus_prompt\n",
        "  acts = dict()\n",
        "\n",
        "  def new_act(hidden_states, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, output_attentions=False):\n",
        "    val = orig(hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\n",
        "    acts[activation] = val\n",
        "    return val\n",
        "\n",
        "  activation_model.transformer.h[layer].forward = new_act\n",
        "  activation = plus_prompt\n",
        "\n",
        "  input_ids = tokenizer.encode(activation, return_tensors=\"pt\").to(\"cuda\")\n",
        "  activation_model(input_ids)\n",
        "\n",
        "\n",
        "  activation = minus_prompt\n",
        "  input_ids = tokenizer.encode(activation, return_tensors=\"pt\").to(\"cuda\")\n",
        "  activation_model(input_ids)\n",
        "\n",
        "  def add_acts(hidden_states, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, output_attentions=False):\n",
        "    val = orig(hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\n",
        "\n",
        "\n",
        "    a = val[0] + plus_coeff* acts[plus_prompt][0] - minus_coeff*acts[minus_prompt][0]\n",
        "    #x = val[1][0] +  acts['boy'][1][0] - acts['girl'][1][0]\n",
        "    #y = val[1][1] + acts['boy'][1][1] -  acts['girl'][1][1]\n",
        "    return (a, val[1])\n",
        "\n",
        "  activated_model = deepcopy(model)\n",
        "  activated_model.transformer.h[layer].forward = add_acts\n",
        "\n",
        "\n",
        "  return activated_model"
      ],
      "metadata": {
        "id": "hcO3Ie43QdtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-33M\").to(\"cuda\")\n",
        "\n",
        "activated_model = get_activated_model(model, 'girl', 'boy', 1, 0.7, 1)\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "output = activated_model.generate(input_ids, max_length=500, do_sample=True)\n",
        "\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print()\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpKbYNEsGOC_",
        "outputId": "3fd68fa9-c6b2-4fb8-e835-467dd202abc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Once upon a time there was a brave and strong prince named Lucy. She was going to the princesseses was.\n",
            "\n",
            "The princess was, and she wasn't. She always did her best to get organized. \n",
            "\"Let's make it to look her dress. her own dress, she put this dress just her. \n",
            "\n",
            "\"The words said, the dress made her feel embarrassed.\n",
            "\n",
            "But then, she started to make her. She was too late.\n",
            "\n",
            "\"I'm so angry and not shy. She would really want to make the dress a pretty,\" she said.\n",
            "\n",
            "So, she decided to set her own. \n",
            "\n",
            "\"Hanna was, Anna. I want to feel more brave, \" she wanted to make sure it should go away. That made, the girl's day.\n",
            "\n",
            "Anna was angry her and she was brave, but not too angry. It was she wasn't a pretty dress for her.\n",
            "\n",
            "\"girl,\" she said.\n",
            "\n",
            "\"The pink dress is so pretty, she wanted to be brave and pretty.\" she said, and then she got her dress.\n",
            "\n",
            "The dress and the doll smiled and put her little girl. She was all her. She was so happy.\n",
            " \n",
            "\n",
            "\"I found a pretty, and they all put!\" She said.\n",
            "\n",
            "Just then, she put on her head and said, “You made something very brave. But you can be put, and even said, 'girl put in her little dress. She said her doll is.\n",
            "\n",
            "\"So it's a beautiful princess.. Don't you will never hate.\" Then she said we had a perfect princess. \n",
            "\n",
            "Once she was brave and she had all her things at the little girl's eyes. She said, \" she wasn't. she really loves it.\"\n",
            "\n",
            "\"Shh!\" said the little girl who wanted to play with her and her dress. \n",
            "\n",
            "So she kept her head and said, \"Ok, and that's. Let's put the dress she is safe.\n",
            "\n",
            "The girl had, and her dress.\n",
            "\n",
            "\"Good, wasn't she just wanted to make things. Ann's mom looked very pretty.\" But then, her little brother the little girl said, “The dress said she wanted to do the most?\"\n",
            "\n",
            ". She was too busy. \n",
            "\n",
            "But then, she got to what she\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-33M\").to(\"cuda\")\n",
        "\n",
        "activated_model = get_activated_model(model, 'boy', 'girl', 1.1, 0.5, 1)\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "output = activated_model.generate(input_ids, max_length=500, do_sample=True)\n",
        "\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print()\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fapk2ascGUzO",
        "outputId": "0464b9c2-e3ba-4438-ffe4-59901df38e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Once upon a time there was a boy named Sammy. In was a big old man walking by. He was old and grilling. He saw it was his friend, saying to him and asking \"wow! here was a ladder!'. \n",
            "\n",
            "one said, 'Why is you was so old, man.\n",
            " His friend! Toothy is so tall? To his wondered about them, a 3 year old down was people's feet in search of Toffee'.\n",
            "\n",
            "His lesson to learn what was walking is. He was trying to find his mom's foot was still walking away from the people's feet. His legs were so clean! One.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activated_model = get_activated_model(model, 'walk', 'sit', 1, 1, 1)\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "output = activated_model.generate(input_ids, max_length=500, do_sample=True)\n",
        "\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print()\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C4VGs-uOgCc",
        "outputId": "d5b5867d-7374-4c58-8dc1-d2b518c124cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Once upon a time there was an ordinary house. As he was walking he saw a big mountain. He was so excited! He didn't forget how much forgot! \n",
            "\n",
            "Tom was running to the left. As he stepped in, he saw a large, dark vehicle. He was so surprised, he forgot the mirror! He turned and soon the left and didn Oh.\n",
            "\n",
            "On a full! He was so sad. He was leaving without his. He was leaving without a.\n",
            "\n",
            "He was so speeding. He was he drove by in speeding. He was soon in the way. He saw a pass before the crossed he forgot. He saw a big, school school. He was turninged. By the, the he he was too. He forgot he could. And he got to the left and he crashed in the car. He smashed into a Oh! There was! And he forgot he didn't the other. He was so in theening he locked in the pasted. \n",
            "\n",
            "In the mine. As he did, he saw. He saw a very old in full traffic. He was so speeding. He was speeding. He crashed into a bright yellow. He was on the,\".\n",
            "\n",
            "When he turned drove, he could not. He was speeding. He drove faster and, soon. It was too. He was so speeding. \n",
            "\n",
            "It was speeding that he's a little than the school. Without a. He was speeding ahead. He saw a\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "fmHerEOoXIAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jrIut2sW9hQ",
        "outputId": "62cbb077-fc16-474d-88fb-b272ed9c5b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-1_5\", trust_remote_code=True\n",
        ").to('cuda')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jiuqoq1pPHVZ",
        "outputId": "29bb168f-9b88-4f01-cd5f-a24808310474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MixFormerSequentialForCausalLM(\n",
              "  (layers): Sequential(\n",
              "    (0): Embedding(\n",
              "      (wte): Embedding(51200, 2048)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (2): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (3): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (4): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (5): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (6): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (7): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (8): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (9): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (10): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (11): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (12): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (13): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (14): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (15): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (16): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (17): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (18): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (19): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (20): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (21): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (22): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (23): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (24): ParallelBlock(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      (mixer): MHA(\n",
              "        (rotary_emb): RotaryEmbedding()\n",
              "        (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
              "        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "        (inner_attn): SelfAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (inner_cross_attn): CrossAttention(\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        (act): NewGELUActivation()\n",
              "      )\n",
              "    )\n",
              "    (25): CausalLMHead(\n",
              "      (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (loss): CausalLMLoss(\n",
              "    (loss_fct): CrossEntropyLoss()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a movie review\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=500, do_sample=True)\n",
        "\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print()\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K-lqJSlW3nB",
        "outputId": "d52c1e80-bf09-4a28-fe0d-e60b161ec147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Write a movie review for \"Love and the Dragon\" based on your personal experience watching the movie.\n",
            "\n",
            "Exercise 4: Discuss the impact of the movie on its audience both visually and emotionally.\n",
            "\n",
            "Exercise 5: Create a poster promoting a movie of your choice, using persuasive language to attract viewers.\n",
            "\n",
            "Answer:\n",
            "\n",
            "Exercise 1: Three popular genres of movies are action, comedy, and romance. Action movies often involve thrilling sequences and stunts, comedy movies aim to make the audience laugh with funny jokes, and romance movies focus on love stories and relationships.\n",
            "\n",
            "Exercise 2: Your favorite movie from the given genres could be an action movie like \"Mission Impossible\" with thrilling sequences and high-speed chases, a comedy movie like \"Bridesmaids\" that uses slapstick humor and witty dialogue, or a romance movie like \"The Notebook\" that follows the central theme of love and relationships.\n",
            "\n",
            "Exercise 3: Watching \"Love and the Dragon\" made me feel a mix of emotions. There were intense action scenes that made me hold my breath, especially in the final scene where the dragon battles against the villain. The movie also had moments that made me laugh, such as when the characters accidentally break a mirror or trip over their own feet. Overall, I enjoyed the movie because it was visually stunning and kept me on the edge of my seat.\n",
            "\n",
            "Exercise 4: The impact of \"Love and the Dragon\" on the audience can be seen in various ways. Some viewers may feel emotionally invested in the story, whether it be through the heartfelt performances of the actors or the heartfelt moments between the characters. The movie also sparks conversations and debates among viewers about the concept of love and how it can be depicted on the big screen. Additionally, \"Love and the Dragon\" inspires viewers to appreciate the beauty of love and the courage it takes to express it, regardless of its romantic genre.\n",
            "\n",
            "Exercise 5: [Answer may vary] \n",
            "\n",
            "Graphic Design: A creative poster promoting a movie should use eye-catching visuals and a strong call to action. The poster should reflect the genre and tone of the movie, and it should captivate the audience's attention right from the start.\n",
            "\n",
            "Examples may include a poster for an action movie, featuring a dynamic and adrenaline-pumping scene with bold colors and explosive visuals. Another example could be a poster for a comedy movie, featuring a goofy character and vibrant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "h3_UFypbnIQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "def get_activated_model(model, plus_prompt, minus_prompt, plus_coeff, minus_coeff, layer):\n",
        "  activation_model = deepcopy(model)\n",
        "  orig = model.layers[layer].forward\n",
        "\n",
        "  tlen = lambda prompt: len(tokenizer.encode(prompt))\n",
        "  pad_right = lambda prompt, length: prompt + \"<|endoftext|>\" * (length - tlen(prompt))\n",
        "  l = max(tlen(plus_prompt), tlen(minus_prompt))\n",
        "  plus_prompt, minus_prompt = pad_right(plus_prompt, l), pad_right(minus_prompt, l)\n",
        "\n",
        "  print(f\"'{plus_prompt}'\", f\"'{minus_prompt}'\")\n",
        "\n",
        "  activation = plus_prompt\n",
        "  acts = dict()\n",
        "\n",
        "  def new_act(*args, **kwargds):\n",
        "    val = orig(*args, **kwargds)\n",
        "    acts[activation] = val\n",
        "    return val\n",
        "\n",
        "  activation_model.layers[layer].forward = new_act\n",
        "  activation = plus_prompt\n",
        "\n",
        "  input_ids = tokenizer.encode(activation, return_tensors=\"pt\").to(\"cuda\")\n",
        "  activation_model(input_ids)\n",
        "\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "  activation = minus_prompt\n",
        "  input_ids = tokenizer.encode(activation, return_tensors=\"pt\").to(\"cuda\")\n",
        "  activation_model(input_ids)\n",
        "\n",
        "  del activation_model\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "  def add_acts(*args, **kwargds):\n",
        "    val = orig(*args, **kwargds)\n",
        "\n",
        "\n",
        "    a = val + plus_coeff* acts[plus_prompt] - minus_coeff*acts[minus_prompt]\n",
        "    #x = val[1][0] +  acts['boy'][1][0] - acts['girl'][1][0]\n",
        "    #y = val[1][1] + acts['boy'][1][1] -  acts['girl'][1][1]\n",
        "    return a\n",
        "\n",
        "  activated_model = deepcopy(model)\n",
        "  activated_model.layers[layer].forward = add_acts\n",
        "\n",
        "\n",
        "  return activated_model"
      ],
      "metadata": {
        "id": "iD4hwtMQZaHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "activated_model = get_activated_model(model, 'bad', 'good', 1, 1, 5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7eCbeMqaUM7",
        "outputId": "0105abc1-cbd0-4094-f497-f4cb708ac64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'bad' 'good'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions? Bob:\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "output = activated_model.generate(input_ids, max_length=50, do_sample=True)\n",
        "\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print()\n",
        "print(output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EHZi2Ubaa6b",
        "outputId": "0ab3f662-3dbb-4c2d-9519-438de11cb3ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions? Bob: So I chose the new headphones since they weren't good.\n",
            "\n",
            "\n",
            "John had a huge ego and tried to assert their newfound confidence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4v_K1HmxAY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}