{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/fractal_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7131dc4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7131dc4d",
        "outputId": "abc6b3a1-604c-4ec6-d982-0459efc80b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-14 12:37:16--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-08-14 12:37:16 (97.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "1f30df8b",
      "metadata": {
        "id": "1f30df8b"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "hfjOrlczPnZ3"
      },
      "id": "hfjOrlczPnZ3",
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6ef9ed1f",
      "metadata": {
        "id": "6ef9ed1f"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "90997e9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90997e9e",
        "outputId": "ee9f2a95-4286-4195-a48b-a5fcde4f6046"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a9fb3378f70>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1137)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "b2a585da",
      "metadata": {
        "id": "b2a585da"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)).to(device))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MulitHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, dropout=0):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(x))\n",
        "        return out\n",
        "\n",
        "class FractalBlock(nn.Module):\n",
        "    def __init__(self,  n_embed, n_head, n_cols, dropout=0):\n",
        "        super().__init__()\n",
        "        self.n_cols = n_cols\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.columns = nn.ModuleList([nn.ModuleList() for _ in range(n_cols)])\n",
        "        self.max_depth = 2 **(n_cols-1)\n",
        "        dist = self.max_depth\n",
        "        self.count = [0] *self.max_depth\n",
        "        for col in self.columns:\n",
        "            for i in range(self.max_depth):\n",
        "                if (i+1)%dist == 0:\n",
        "                    module = MulitHeadAttention(n_head, n_embed//n_head)\n",
        "                    self.count[i]+=1\n",
        "                else:\n",
        "                    module = None\n",
        "                col.append(module)\n",
        "            dist //= 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = [x for _ in range(self.n_cols)]\n",
        "        for i in range(self.max_depth):\n",
        "            st = self.n_cols - self.count[i]\n",
        "            cur_outs = []\n",
        "            for c in range(st, self.n_cols):\n",
        "                cur_in = out[c]\n",
        "                cur_module = self.columns[c][i]\n",
        "                cur_outs.append(cur_module(cur_in))\n",
        "\n",
        "            n_out = torch.stack(cur_outs)\n",
        "\n",
        "            n_out = n_out.mean(dim=0)\n",
        "\n",
        "            for c in range(st, self.n_cols):\n",
        "                out[c] = n_out\n",
        "        return self.dropout(out[-1])\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed, dropout=0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4* n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "         nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head, n_cols):\n",
        "        super().__init__()\n",
        "        self.sa_head= FractalBlock(n_embed, n_head, n_cols )\n",
        "        self.ffw=  FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa_head(self.ln1(x))\n",
        "        x = x + self.ffw(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FractalTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed).to(device)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed).to(device)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, n_cols=4) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx).to(device)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokes):\n",
        "        for _ in range(max_new_tokes):\n",
        "            idx_cond = idx[:, -block_size:].to(device)\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim = -1)\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "            idx = torch.cat((idx, idx_next), dim = 1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "24122aa8",
      "metadata": {
        "id": "24122aa8"
      },
      "outputs": [],
      "source": [
        "model = FractalTransformer()\n",
        "model  = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "id": "18a31597",
      "metadata": {
        "id": "18a31597"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X = X.to(device)\n",
        "            Y=Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "id": "fb0fd57b",
      "metadata": {
        "id": "fb0fd57b"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "n_embed = 32\n",
        "n_head = 1\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "id": "fe443047",
      "metadata": {
        "id": "fe443047"
      },
      "outputs": [],
      "source": [
        "model = FractalTransformer()\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "id": "49c6d83f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49c6d83f",
        "outputId": "6e46c24d-9ef7-478c-b9ce-be9f2d762ac3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "169893"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ],
      "source": [
        "len(text.split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "id": "177f3d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "177f3d2d",
        "outputId": "34fc175b-f306-464c-cbef-599056029c2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "id": "69c21edc",
      "metadata": {
        "id": "69c21edc"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text.split(' '))))\n",
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "4871f881",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4871f881",
        "outputId": "36af52a6-66b0-4954-eecb-04621e7d46eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWas'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 183
        }
      ],
      "source": [
        "chars[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "8723537f",
      "metadata": {
        "id": "8723537f"
      },
      "outputs": [],
      "source": [
        "stoi = { ch:i for i, ch in enumerate(chars) }\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \" \".join([itos[x] for x in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "id": "27375acc",
      "metadata": {
        "id": "27375acc"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text.split(' ')), dtype = torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "f04ec236",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f04ec236",
        "outputId": "103db7b0-26ce-4b1a-ad6f-d9fc6da2e86c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1455,   957, 39874, 29614,  5949, 16628, 18572, 24432, 34050, 34057])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "id": "7ef6e940",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7ef6e940",
        "outputId": "a20335d8-051a-4451-9765-a173a361f5ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "decode(encode(text.split(' ')[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "7601b1ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7601b1ab",
        "outputId": "7cfb73b7-4643-45c7-fd54-300e7d367908"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['First',\n",
              " 'Citizen:\\nBefore',\n",
              " 'we',\n",
              " 'proceed',\n",
              " 'any',\n",
              " 'further,',\n",
              " 'hear',\n",
              " 'me',\n",
              " 'speak.\\n\\nAll:\\nSpeak,',\n",
              " 'speak.\\n\\nFirst']"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ],
      "source": [
        "text.split(' ')[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "id": "bb845c85",
      "metadata": {
        "id": "bb845c85"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "id": "53fce7e1",
      "metadata": {
        "id": "53fce7e1"
      },
      "outputs": [],
      "source": [
        "model = FractalTransformer()\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "8bd7ebd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bd7ebd6",
        "outputId": "d47b44e1-2d06-4aae-bd5c-80f9a2425029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 11.0261, val loss 11.0367\n",
            "step 100: train loss 8.1894, val loss 8.3922\n",
            "step 200: train loss 8.0557, val loss 8.3469\n",
            "step 300: train loss 7.9869, val loss 8.3925\n",
            "step 400: train loss 7.9350, val loss 8.4275\n",
            "step 500: train loss 7.8715, val loss 8.4019\n",
            "step 600: train loss 7.7944, val loss 8.4512\n",
            "step 700: train loss 7.7152, val loss 8.4584\n",
            "step 800: train loss 7.6717, val loss 8.4280\n",
            "step 900: train loss 7.5592, val loss 8.4500\n",
            "step 999: train loss 7.4792, val loss 8.4682\n"
          ]
        }
      ],
      "source": [
        "max_iters = 1000\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % 100 == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb = xb.to(device)\n",
        "    yb  = yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "id": "58b6580e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58b6580e",
        "outputId": "58470d9a-ede7-4668-f695-bfb3b00ee0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thou art kneel before king unavoided way\n",
            "and my sight!\n",
            "\n",
            "Nurse:\n",
            "O out of wondrous linen. poor toward you.\n",
            "Be what to instruct united grievous straight den.\n",
            "Whose one day, at strike honour.\n",
            "\n",
            "ANGELO:\n",
            "See do, leave no.\n",
            "\n",
            "BRUTUS:\n",
            "The pities all shade you, do II:\n",
            "A in long she father. hang speaks than thy tribunes; exile, monastery or lost, thou away.\n",
            "3 speak:\n",
            "Look when it ell quoth and thank Hereford, forfeits. though no lived!\n",
            "A spectacle him?\n",
            "\n",
            "DUKE death?\n",
            "What, secrecy. fair, Menenius, Gaunt,\n",
            "Even will eats now, given what's said will blot?\n",
            "\n",
            "Abbot:\n",
            "My our eyes time\n",
            "Unfold full you are gold's and no palsy, VI\n",
            "\n",
            "GLOUCESTER:\n",
            "Now navy speak; there: your even: them\n",
            "For for draw houses!\n",
            "\n",
            "ROMEO:\n",
            "This thee a years,\n",
            "Pass'd foretell but them.\n",
            "\n",
            "LUCIO:\n",
            "Friar, after,\n",
            "Is II:\n",
            "Needs forth;\n",
            "My thankful, think\n",
            "That in dangerous\n",
            "to 'gainst with\n",
            "Hath and If a lordship France, how our fellows:\n",
            "He had hands watch than princely free of his instruction can oft thy approbation.\n",
            "\n",
            "CORIOLANUS:\n",
            "Where? strength; think you and joy not tied may Poor now he's kings,\n",
            "Beseeching thee; himself 'gainst retired lament\n",
            "The Go: lives at condemn'd the fares:\n",
            "By wounds here And, no him.\n",
            "\n",
            "SICINIUS:\n",
            "Say, stand the hell!\n",
            "All this Romeo, him your Roman now graced care father,\n",
            "To dead whom death.\n",
            "\n",
            "DUKE Jove's long.\n",
            "As he speak parties, awake\n",
            "Your deal a pleasure.\n",
            "\n",
            "DUKE to be thy conversation words; neither\n",
            "good what thou but dead? intends;\n",
            "And worships! fault's silent been Publius you dolours\n"
          ]
        }
      ],
      "source": [
        "context = torch.tensor([encode(\"thou art kneel before king\".split(' '))], dtype = torch.long).to(device)\n",
        "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "6244a62f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6244a62f",
        "outputId": "168a330c-9350-44a8-8586-f4565d8cf43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thou art kneel before king if for brother, his sheets,\n",
            "Which but our hale glassy shall makes by necessity;\n",
            "For might me;\n",
            "No, not thou art like sighs us sends\n",
            "It IV:\n",
            "Why, the kissing, one dies.\n",
            "\n",
            "CLAUDIO:\n",
            "Why slander comes courage souls.\n",
            "\n",
            "First once.\n",
            "Come way,\n",
            "Which as circled on on!\n",
            "\n",
            "CORIOLANUS:\n",
            "\n",
            "VOLUMNIA:\n",
            "I one is farewell.\n",
            "\n",
            "EARL lo, of tongue?\n",
            "\n",
            "Messenger:\n",
            "Ah, to save time that to you hear. will, do pursue hath if not so and away, can here; expiring in our war\n",
            "Their put along,\n",
            "Holding prithee, be certain for York!\n",
            "Suppose were at plainly the thorn.\n",
            "\n",
            "DUKE Stanley;\n",
            "Oxford, I jest;\n",
            "His YORK:\n",
            "Blind me spouts: nothing?\n",
            "Why, to thee: or you do?\n",
            "\n",
            "ISABELLA:\n",
            "As the rigour again of death,\n",
            "If shall not would them, by let me how year to heir king the certain.\n",
            "\n",
            "DUKE a dearest noble swerve, state, sworn of all victory in the petition\n",
            "well my sweet voices? upon\n",
            "Show that noble Marian complaints\n",
            "All brother false vessel down with men,\n",
            "From not a other\n",
            "station; Lucentio burst, 'tis by the traitor one Lancaster, lap of holy and if stinkingly and shop, seas;\n",
            "The me them meanly at\n",
            "Your ceremonious voices yours;\n",
            "Nor thus.\n",
            "Stamp, to have encounter unto your extremes send thy she?\n",
            "\n",
            "GLOUCESTER:\n",
            "What, we am KING not, his confessor, so entreat gorgeous you island--\n",
            "Save so: me tell a other make his contents of the orisons not.\n",
            "\n",
            "MENENIUS:\n",
            "I and as our alone;\n",
            "I'll you mean\n"
          ]
        }
      ],
      "source": [
        "context = torch.tensor([encode(\"thou art kneel before king\".split(' '))], dtype = torch.long).to(device)\n",
        "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "id": "35b54cce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35b54cce",
        "outputId": "5ce1fa96-79e1-4ee0-94e9-2773ed047cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hermione of men.\n",
            "\n",
            "Second proud;\n",
            "But, be friends.\n",
            "Ourself the dullest be fault\n",
            "I' will Eve, knight, that command be tasted, O looks desire of her be\n",
            "called had Officer:\n",
            "Faith, it fall and awake, occasion where in Helena.' your tent\n",
            "I'll a heart.\n",
            "Those showing hadst how they do more: Watchman:\n",
            "To-morrow me Tybalts. than\n",
            "his there,\n",
            "With my garland. with it shall Tarquin's the truth, king;\n",
            "Of you, and with the while!\n",
            "\n",
            "Third is a rising this thee i' this witless Gloucester's put what that dim had witnesses and the be?--\n",
            "With about this heed bounty. in youth\n",
            "There you.\n",
            "\n",
            "ANGELO:\n",
            "Teach the house a prate,\n",
            "And Clarence sighs i' desire down.\n",
            "\n",
            "Servant:\n",
            "What, man, RICHARD mouths O be wisdom my you?\n",
            "\n",
            "CORIOLANUS:\n",
            "I to the right; gnaw'd safe.\n",
            "\n",
            "DUKE Ere further.\n",
            "\n",
            "CORIOLANUS:\n",
            "Ha! OF soldier.\n",
            "Ah, peevish night!\n",
            "This take\n",
            "The so Richard's love,\n",
            "Can melancholy\n",
            "Hath on them, when I away! duty having, me lightness? humbly but near.\n",
            "Go, smile I know a power tears depends\n",
            "Upon to. revenge would took up doth, you Caesar's composition, is my name? go.\n",
            "\n",
            "LORD my supply deed little not your night.\n",
            "\n",
            "DUCHESS stand long him he, me slide: mad? off, spices to piece you hoping\n",
            "To shall laws,\n",
            "She not?\n",
            "\n",
            "Gaoler:\n",
            "For found? GREY:\n",
            "That's die KING myself,\n",
            "Makes subject with this disobedient to colour me sackbuts, though from?\n",
            "O, we would blood:\n",
            "I'll and VI:\n",
            "The wherefore leave it.\n",
            "\n",
            "MENENIUS:\n",
            "All's liege, Camillo,\n",
            "May thee\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "context = torch.tensor([encode(\"Hermione\".split(' '))], dtype = torch.long).to(device)\n",
        "print(decode(model.generate(context, max_new_tokes=200)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bq-gw0jcQ5-p"
      },
      "id": "bq-gw0jcQ5-p",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}