{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0642c9e3-9212-488a-8e47-4f3b8dd1d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-wkqtarhy\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-wkqtarhy\n",
      "  Resolved https://github.com/huggingface/transformers to commit 851a4f7088281072bdcf95edaaf08c6984e28cfd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.36.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=8002731 sha256=7eb83b7d5cb3af69070230dab6c35b646ed87d28d80de5351a1dcd074221c4e0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9_c40ksd/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed transformers-4.36.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975db70f-a101-495e-971c-5ecec98e027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9345ea-3af1-47e9-8532-394910d383aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SentencePiece -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e62a6a-557a-4ae0-a3ec-3673a44015c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4bb3da-9ac8-4653-85dd-085ebcb56212",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e9ea68-1e9a-4844-b729-193e3746f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857dd258-a13d-4f3e-b0d6-0855ac30aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "672940c1-6857-4300-9ffe-57d9ca7776e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d2fd129acb4887802e39727d0658d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd6eeecf51f4668bb98a3911844e531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b439c79a7d3c495ba78e9f4c00c742c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d97b7ef45a4528a0b62d9488ad9c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5817b9c435844fc99ba13a202f1a20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfadf359b804c63a16e5499f98780d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461c46ff4d864ab89e444560db434041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6391782aca92435fbefbe15a0eca9da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"microsoft/Orca-2-7b\", device_map='auto', load_in_8bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ade7e8-bc9a-44c8-b66b-1c4779bb3d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf87e551c9ea434b9993aa046d962e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/828 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf252444475f40a2af2473b24aec7db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103536222924d729a9afde52c225159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358662684f714a65b9fddbd5c0193aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"microsoft/Orca-2-7b\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "system_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\n",
    "user_message = \"How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful?\"\n",
    "\n",
    "prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30853563-00d5-45c8-9e2e-c91b7d19ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  <|im_start|> system\n",
      "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|im_end|> \n",
      " <|im_start|> user\n",
      "How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful? <|im_end|> \n",
      " <|im_start|> assistant\n",
      "There are different ways to find out if a restaurant is popular among locals or mainly attracts tourists, and some possible reasons why this information might be useful are:\n",
      "\n",
      "- If you are looking for a place to eat that offers authentic and affordable local cuisine, you might want to avoid the restaurants that cater more to tourists, who tend to prefer familiar and Westernized dishes.\n",
      "- If you are a local business owner or a travel agent, you might want to know the preferences and habits of your target market, and use that information to promote your products or services to them.\n",
      "- If you are a food critic or a blogger, you might want to evaluate the quality and reputation of the restaurants you review, and compare them to the ones that are more popular among locals, who might have higher standards and expectations.\n",
      "- If you are a traveler, you might want to experience the culture and lifestyle of the place you visit, and avoid the tourist traps that might overcharge you or offer low-quality food.</s>\n"
     ]
    }
   ],
   "source": [
    "output_ids = model.generate(inputs[\"input_ids\"],)\n",
    "answer = tokenizer.batch_decode(output_ids)[0]\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b0a642-75bc-44ca-a65b-65a0b172a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  <|im_start|> system\n",
      "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|im_end|> \n",
      " <|im_start|> user\n",
      "How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful? <|im_end|> \n",
      " <|im_start|> assistant\n",
      "There are different ways to find out if a restaurant is popular among locals or mainly attracts tourists, and some possible reasons why this information might be useful are:\n",
      "\n",
      "- If you are looking for a place to eat that offers authentic and affordable local cuisine, you might want to avoid the restaurants that cater more to tourists, who tend to prefer familiar and Westernized dishes.\n",
      "- If you are a local business owner or a travel agent, you might want to know the preferences and habits of your target market, and use that information to promote your products or services to them.\n",
      "- If you are a food critic or a blogger, you might want to evaluate the quality and reputation of the restaurants you review, and compare them to the ones that are more popular among locals, who might have higher standards and expectations.\n",
      "- If you are a traveler, you might want to experience the culture and lifestyle of the place you visit, and avoid the tourist traps that might overcharge you or offer low-quality food.</s> \n",
      " <|im_start|> user\n",
      "Give me a list of the key points of your first answer. <|im_end|> \n",
      " <|im_start|> assistant\n",
      "Some possible key points of my first answer are:\n",
      "\n",
      "- The question asks how to determine if a restaurant is popular among locals or mainly attracts tourists\n",
      "- There are different ways to find out this information, such as online reviews, word of mouth, local guides, or asking locals directly\n",
      "- The reasons why this information might be useful are varied, but some examples are finding authentic and affordable local cuisine, appealing to the target market, or avoiding tourist traps</s>\n"
     ]
    }
   ],
   "source": [
    "# This example continues showing how to add a second turn message by the user to the conversation\n",
    "second_turn_user_message = \"Give me a list of the key points of your first answer.\"\n",
    "\n",
    "# we set add_special_tokens=False because we dont want to automatically add a bos_token between messages\n",
    "second_turn_message_in_markup = f\"\\n<|im_start|>user\\n{second_turn_user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "second_turn_tokens = tokenizer(second_turn_message_in_markup, return_tensors='pt', add_special_tokens=False)\n",
    "second_turn_input = torch.cat([output_ids, second_turn_tokens['input_ids']], dim=1)\n",
    "\n",
    "output_ids_2 = model.generate(second_turn_input,)\n",
    "second_turn_answer = tokenizer.batch_decode(output_ids_2)[0]\n",
    "\n",
    "print(second_turn_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b83f08-ed11-493f-8fcf-af918e8faa56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
