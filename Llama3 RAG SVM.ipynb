{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d328722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.retrievers import SVMRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.retrievers import SVMRetriever\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ccf59b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\", groq_api_key='')\n",
    "\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    \n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"agent memory\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "328f71bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/pradeep.borado/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "embeddings = GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32689369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "class Retriever():\n",
    "    def __init__(self, docs, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.docs = docs\n",
    "        x = [doc_split.page_content for doc_split in docs]\n",
    "        embeds = embeddings.embed_documents(x)\n",
    "        embeds_np = np.array(embeds)\n",
    "        embeds_np = embeds_np / np.sqrt((embeds_np**2).sum(1, keepdims=True)) # L2 normalize the rows, as is common\n",
    "        self.embeds = embeds_np\n",
    "        \n",
    "    def query(self, question, k=5):\n",
    "        query = np.array(self.embeddings.embed_query(question))\n",
    "\n",
    "        query = query / np.sqrt((query**2).sum())\n",
    "        x = np.concatenate([[query], self.embeds]) \n",
    "        y = np.zeros(len(x))\n",
    "        y[0] = 1 # we have a single positive example, mark it as such\n",
    "\n",
    "        # train our (Exemplar) SVM\n",
    "        # docs: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "        clf = svm.LinearSVC(class_weight='balanced', verbose=False, max_iter=50000, tol=1e-5, C=1)\n",
    "        clf.fit(x, y) # train\n",
    "\n",
    "        # infer on whatever data you wish, e.g. the original data\n",
    "        similarities = clf.decision_function(x)\n",
    "        sorted_ix = np.argsort(-similarities)[1:]\n",
    "        res = []\n",
    "        for i in sorted_ix[:k]:\n",
    "            res.append(self.docs[i-1])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ac6d6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(doc_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ed76ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The components of an agent system include:\n",
      "\n",
      "1. Planning: Task decomposition and self-reflection.\n",
      "2. Memory: Types of memory, such as Maximum Inner Product Search (MIPS), and memory stream.\n",
      "3. Tool Use: Case studies, such as scientific discovery agent and generative agents simulation.\n"
     ]
    }
   ],
   "source": [
    "q = 'What are the components of a agent system'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e55f6b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent's memory is a long-term memory module that records a comprehensive list of agents' experiences in natural language.\n"
     ]
    }
   ],
   "source": [
    "q = 'agent memory'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ad048089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a process of breaking down a complex task into smaller, more manageable subtasks. This is often necessary when dealing with complex problems that require multiple steps to solve. In the context of LLM-powered autonomous agents, task decomposition is used to decompose a task into smaller, more manageable steps that the LLM can understand and execute. This can be done through various methods, such as using simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", or using task-specific instructions.\n"
     ]
    }
   ],
   "source": [
    "q='Explain Task Decomposition in depth?'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "27575e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of thought (CoT) prompting generates a sequence of short sentences to describe reasoning logics step by step, known as reasoning chains or rationales, to eventually lead to the final answer. This technique is more pronounced for complicated reasoning tasks, while using large models (e.g., with more than 50B parameters).\n"
     ]
    }
   ],
   "source": [
    "q='Explain chain of thought?'\n",
    "d = retriever.query(q)\n",
    "generation = rag_chain.invoke({\"context\": d, \"question\": q})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa6e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
