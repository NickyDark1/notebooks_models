{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76bfcbfb-a13d-4750-ad9f-9055fcb0329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rwkv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff19798f-d777-46c8-a064-338f97accc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758756c4-e1a8-4f48-b24d-fffa9d146a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['RWKV_JIT_ON'] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55977ae-b1f7-4d61-b6f1-7fd8bbd4e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-29 14:11:50--  https://huggingface.co/RWKV/v5-Eagle/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth\n",
      "Resolving huggingface.co (huggingface.co)... 13.32.110.55, 13.32.110.77, 13.32.110.28, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.32.110.55|:443... connected.\n",
      "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
      "Location: /RWKV/v5-Eagle-7B/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth [following]\n",
      "--2024-01-29 14:11:51--  https://huggingface.co/RWKV/v5-Eagle-7B/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth\n",
      "Reusing existing connection to huggingface.co:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/d5/6f/d56f8718b68e0e1840ad1e209498db64132d773e8c85a1bf4f194501bc3cddcf/a88c7274184b211e5545c8f992f0b80d03c40a447980bbfcd0f6d5858982615a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth%3B+filename%3D%22RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth%22%3B&Expires=1706796711&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc5NjcxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Q1LzZmL2Q1NmY4NzE4YjY4ZTBlMTg0MGFkMWUyMDk0OThkYjY0MTMyZDc3M2U4Yzg1YTFiZjRmMTk0NTAxYmMzY2RkY2YvYTg4YzcyNzQxODRiMjExZTU1NDVjOGY5OTJmMGI4MGQwM2M0MGE0NDc5ODBiYmZjZDBmNmQ1ODU4OTgyNjE1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Xpj9EJrmM%7EKnSQL2yl-o9wmYvEDvh61SF0bjD2E06hEMjDSmk%7ElbZayJKxUgdhEQSwzugjH6kY4oIAU0OjvSyZZQT7jUZR20XlHhMSXplhWgWIqvBspfRVmwfuFWdG4kcNzeJOKV6LX58LgYT%7EjdsWvO%7E-irfrwy753NC0p6B2wOpCBFHYI2ljRunnZBjSABftRaEETAtt1G5G44ot4O1ANRR4UBNRHLbtjyJCVLPamz1lZWTaSLNyLfuKU8XnCMENvpK8ohaRzSNV7D8SLems-dAj0KIzYmWsmCzOYKN6vQWQD0b23bGEVhrBoV-2L5KKQ8OPZJbgF9qtu-c%7EOy6Q__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-01-29 14:11:51--  https://cdn-lfs-us-1.huggingface.co/repos/d5/6f/d56f8718b68e0e1840ad1e209498db64132d773e8c85a1bf4f194501bc3cddcf/a88c7274184b211e5545c8f992f0b80d03c40a447980bbfcd0f6d5858982615a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth%3B+filename%3D%22RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth%22%3B&Expires=1706796711&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjc5NjcxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Q1LzZmL2Q1NmY4NzE4YjY4ZTBlMTg0MGFkMWUyMDk0OThkYjY0MTMyZDc3M2U4Yzg1YTFiZjRmMTk0NTAxYmMzY2RkY2YvYTg4YzcyNzQxODRiMjExZTU1NDVjOGY5OTJmMGI4MGQwM2M0MGE0NDc5ODBiYmZjZDBmNmQ1ODU4OTgyNjE1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Xpj9EJrmM%7EKnSQL2yl-o9wmYvEDvh61SF0bjD2E06hEMjDSmk%7ElbZayJKxUgdhEQSwzugjH6kY4oIAU0OjvSyZZQT7jUZR20XlHhMSXplhWgWIqvBspfRVmwfuFWdG4kcNzeJOKV6LX58LgYT%7EjdsWvO%7E-irfrwy753NC0p6B2wOpCBFHYI2ljRunnZBjSABftRaEETAtt1G5G44ot4O1ANRR4UBNRHLbtjyJCVLPamz1lZWTaSLNyLfuKU8XnCMENvpK8ohaRzSNV7D8SLems-dAj0KIzYmWsmCzOYKN6vQWQD0b23bGEVhrBoV-2L5KKQ8OPZJbgF9qtu-c%7EOy6Q__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.161.119.100, 3.161.119.115, 3.161.119.95, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.161.119.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15036197526 (14G) [binary/octet-stream]\n",
      "Saving to: ‘RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth’\n",
      "\n",
      "RWKV-v5-Eagle-World 100%[===================>]  14.00G   110MB/s    in 2m 21s  \n",
      "\n",
      "2024-01-29 14:14:12 (102 MB/s) - ‘RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth’ saved [15036197526/15036197526]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/RWKV/v5-Eagle/resolve/main/RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d58f48-bae8-4fa2-bd18-0332be83a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv.model import RWKV\n",
    "from rwkv.utils import PIPELINE, PIPELINE_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6e560c-6f22-4264-8d08-1b78215c7324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 0\n",
      "\n",
      "Loading RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth ...\n",
      "Model detected: v5.2\n",
      "Strategy: (total 32+1=33 layers)\n",
      "* cuda [float32, float32], store 33 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 \n",
      "emb.weight                        f32      cpu  65536  4096 \n",
      "blocks.0.ln1.weight               f32   cuda:0   4096       \n",
      "blocks.0.ln1.bias                 f32   cuda:0   4096       \n",
      "blocks.0.ln2.weight               f32   cuda:0   4096       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   4096       \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   4096       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   4096       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   4096       \n",
      "blocks.0.att.time_mix_g           f32   cuda:0   4096       \n",
      "blocks.0.att.time_decay           f32   cuda:0     64    64 \n",
      "blocks.0.att.time_first           f32   cuda:0     64    64 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   4096  4096 \n",
      "blocks.0.att.key.weight           f32   cuda:0   4096  4096 \n",
      "blocks.0.att.value.weight         f32   cuda:0   4096  4096 \n",
      "blocks.0.att.output.weight        f32   cuda:0   4096  4096 \n",
      "blocks.0.att.gate.weight          f32   cuda:0   4096  4096 \n",
      "blocks.0.att.ln_x.weight          f32   cuda:0   4096       \n",
      "blocks.0.att.ln_x.bias            f32   cuda:0   4096       \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   4096       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   4096       \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   4096 14336 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   4096  4096 \n",
      "blocks.0.ffn.value.weight         f32   cuda:0  14336  4096 \n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.31.ln1.weight              f32   cuda:0   4096       \n",
      "blocks.31.ln1.bias                f32   cuda:0   4096       \n",
      "blocks.31.ln2.weight              f32   cuda:0   4096       \n",
      "blocks.31.ln2.bias                f32   cuda:0   4096       \n",
      "blocks.31.att.time_mix_k          f32   cuda:0   4096       \n",
      "blocks.31.att.time_mix_v          f32   cuda:0   4096       \n",
      "blocks.31.att.time_mix_r          f32   cuda:0   4096       \n",
      "blocks.31.att.time_mix_g          f32   cuda:0   4096       \n",
      "blocks.31.att.time_decay          f32   cuda:0     64    64 \n",
      "blocks.31.att.time_first          f32   cuda:0     64    64 \n",
      "blocks.31.att.receptance.weight   f32   cuda:0   4096  4096 \n",
      "blocks.31.att.key.weight          f32   cuda:0   4096  4096 \n",
      "blocks.31.att.value.weight        f32   cuda:0   4096  4096 \n",
      "blocks.31.att.output.weight       f32   cuda:0   4096  4096 \n",
      "blocks.31.att.gate.weight         f32   cuda:0   4096  4096 \n",
      "blocks.31.att.ln_x.weight         f32   cuda:0   4096       \n",
      "blocks.31.att.ln_x.bias           f32   cuda:0   4096       \n",
      "blocks.31.ffn.time_mix_k          f32   cuda:0   4096       \n",
      "blocks.31.ffn.time_mix_r          f32   cuda:0   4096       \n",
      "blocks.31.ffn.key.weight          f32   cuda:0   4096 14336 \n",
      "blocks.31.ffn.receptance.weight   f32   cuda:0   4096  4096 \n",
      "blocks.31.ffn.value.weight        f32   cuda:0  14336  4096 \n",
      "ln_out.weight                     f32   cuda:0   4096       \n",
      "ln_out.bias                       f32   cuda:0   4096       \n",
      "head.weight                       f32   cuda:0   4096 65536 \n"
     ]
    }
   ],
   "source": [
    "model = RWKV(model='RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth', strategy='cuda fp32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b7e18fd-f63d-46fb-be24-8b5696c628c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Write a short story about good food and how it can make people happy.\n",
      "\n",
      "Answer:\n",
      "Once upon a time, there was a small town where the people were always busy with work and hardly had time for anything else. One day, a kind-hearted chef opened a restaurant in the heart of the town, promising to serve the best food ever. \n",
      "At first, people were skeptical. Who was this chef who claimed to be able to make them happy with just food? But soon, they started to hear whispers about how the chef's food was so good that it made their hearts sing. \n",
      "The restaurant became a hotspot, and people from all over the town would come to try the chef's creations. The food was not only delicious but also made people feel happy inside. They would sit for hours, savoring every bite, laughing and chatting with each other. \n",
      "As word spread, more and more people began to visit the restaurant. Soon, it became a destination for foodies from all over the country\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\") # 20B_tokenizer.json is in https://github.com/BlinkDL/ChatRWKV\n",
    "\n",
    "ctx = \"\\n Write a short story about good food\"\n",
    "print(ctx, end='')\n",
    "\n",
    "def my_print(s):\n",
    "    print(s, end='', flush=True)\n",
    "\n",
    "# For alpha_frequency and alpha_presence, see \"Frequency and presence penalties\":\n",
    "# https://platform.openai.com/docs/api-reference/parameter-details\n",
    "\n",
    "args = PIPELINE_ARGS(temperature = 1, top_p = 0.7, top_k = 100, # top_k = 0 then ignore\n",
    "                     alpha_frequency = 0.25,\n",
    "                     alpha_presence = 0.25,\n",
    "                     alpha_decay = 0.996, # gradually decay the penalty\n",
    "                     token_ban = [0], # ban the generation of some tokens\n",
    "                     token_stop = [], # stop generation whenever you see any token here\n",
    "                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)\n",
    "\n",
    "pipeline.generate(ctx, token_count=200, args=args, callback=my_print)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06813873-d503-4459-85d0-cdf014c60174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rwkv.cpp'...\n",
      "remote: Enumerating objects: 2084, done.\u001b[K\n",
      "remote: Counting objects: 100% (679/679), done.\u001b[K\n",
      "remote: Compressing objects: 100% (296/296), done.\u001b[K\n",
      "remote: Total 2084 (delta 467), reused 497 (delta 359), pack-reused 1405\u001b[K\n",
      "Receiving objects: 100% (2084/2084), 17.03 MiB | 22.53 MiB/s, done.\n",
      "Resolving deltas: 100% (1267/1267), done.\n",
      "Submodule 'ggml' (https://github.com/saharNooby/ggml) registered for path 'ggml'\n",
      "Cloning into '/workspace/rwkv.cpp/ggml'...\n",
      "remote: Enumerating objects: 2971, done.        \n",
      "remote: Counting objects: 100% (2971/2971), done.        \n",
      "remote: Compressing objects: 100% (794/794), done.        \n",
      "remote: Total 2971 (delta 2011), reused 2845 (delta 1977), pack-reused 0        \n",
      "Receiving objects: 100% (2971/2971), 4.83 MiB | 20.03 MiB/s, done.\n",
      "Resolving deltas: 100% (2011/2011), done.\n",
      "Submodule path 'ggml': checked out '46f083d15bb31c62933300ffbfffa5aa6ae2ecae'\n",
      "/workspace/rwkv.cpp\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/saharNooby/rwkv.cpp.git\n",
    "%cd rwkv.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e998c23e-4b91-4b6d-b182-04a03762a193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  g++-9 libstdc++-9-dev\n",
      "Suggested packages:\n",
      "  g++-multilib g++-9-multilib gcc-9-doc libstdc++-9-doc\n",
      "The following NEW packages will be installed:\n",
      "  g++ g++-9 libstdc++-9-dev\n",
      "0 upgraded, 3 newly installed, 0 to remove and 19 not upgraded.\n",
      "Need to get 10.1 MB of archives.\n",
      "After this operation, 46.8 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libstdc++-9-dev amd64 9.4.0-1ubuntu1~20.04.2 [1722 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 g++-9 amd64 9.4.0-1ubuntu1~20.04.2 [8421 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 g++ amd64 4:9.3.0-1ubuntu2 [1604 B]\n",
      "Fetched 10.1 MB in 9s (1165 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libstdc++-9-dev:amd64.\n",
      "(Reading database ... 17788 files and directories currently installed.)\n",
      "Preparing to unpack .../libstdc++-9-dev_9.4.0-1ubuntu1~20.04.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libstdc++-9-dev:amd64 (9.4.0-1ubuntu1~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package g++-9.\n",
      "Preparing to unpack .../g++-9_9.4.0-1ubuntu1~20.04.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking g++-9 (9.4.0-1ubuntu1~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Selecting previously unselected package g++.\n",
      "Preparing to unpack .../g++_4%3a9.3.0-1ubuntu2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking g++ (4:9.3.0-1ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Setting up libstdc++-9-dev:amd64 (9.4.0-1ubuntu1~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up g++-9 (9.4.0-1ubuntu1~20.04.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up g++ (4:9.3.0-1ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/c++.1.gz because associated file /usr/share/man/man1/g++.1.gz (of link group c++) doesn't exist\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!sudo apt install g++ -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0753e5f7-de55-4cd7-a711-1d025a2226d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  make-doc\n",
      "The following NEW packages will be installed:\n",
      "  make\n",
      "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
      "Need to get 162 kB of archives.\n",
      "After this operation, 393 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 make amd64 4.2.1-1.2 [162 kB]\n",
      "Fetched 162 kB in 0s (835 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package make.\n",
      "(Reading database ... 18642 files and directories currently installed.)\n",
      "Preparing to unpack .../make_4.2.1-1.2_amd64.deb ...\n",
      "Unpacking make (4.2.1-1.2) ...\n",
      "Setting up make (4.2.1-1.2) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install make -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "749ac412-fdaf-4eb1-aba2-6dc8ed5bfb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 9.4.0\n",
      "-- The CXX compiler identification is GNU 9.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
      "-- Check if compiler accepts -pthread\n",
      "-- Check if compiler accepts -pthread - yes\n",
      "-- Found Threads: TRUE  \n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- x86 detected\n",
      "-- Configuring done (0.7s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /workspace/rwkv.cpp\n",
      "[  4%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml/src/ggml.c.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml/src/ggml-alloc.c.o\u001b[0m\n",
      "[  9%] Built target ggml\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/rwkv.dir/rwkv.cpp.o\u001b[0m\n",
      "In file included from \u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv.cpp:42\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_utilities.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool rwkv_fread_string(FILE*, size_t, std::string&)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_utilities.inc:36:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst char*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kvoid*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K-Wcast-qual\u001b[m\u001b[K]\n",
      "   36 |     return fread((void *) dest.data(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, length, 1, file) == 1;\n",
      "      |                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv.cpp:2\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_tensor* rwkv_wkv_v5(ggml_context*, size_t, size_t, size_t, size_t, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*, ggml_tensor*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:154:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  154 |     GGML_ASSERT(\u001b[01;35m\u001b[Kx->ne[0] == C\u001b[m\u001b[K && x->ne[1] == T && x->ne[2] == 1 && x->ne[3] == 1);\n",
      "      |                 \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:154:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  154 |     GGML_ASSERT(x->ne[0] == C && \u001b[01;35m\u001b[Kx->ne[1] == T\u001b[m\u001b[K && x->ne[2] == 1 && x->ne[3] == 1);\n",
      "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:155:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  155 |     GGML_ASSERT(k->ne[0] == 1 && \u001b[01;35m\u001b[Kk->ne[1] == S\u001b[m\u001b[K && k->ne[2] == H && k->ne[3] == T);\n",
      "      |                                  \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:155:60:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  155 |     GGML_ASSERT(k->ne[0] == 1 && k->ne[1] == S && \u001b[01;35m\u001b[Kk->ne[2] == H\u001b[m\u001b[K && k->ne[3] == T);\n",
      "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:155:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  155 | (k->ne[0] == 1 && k->ne[1] == S && k->ne[2] == H && \u001b[01;35m\u001b[Kk->ne[3] == T\u001b[m\u001b[K);\n",
      "      |                                                     \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:156:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  156 |     GGML_ASSERT(\u001b[01;35m\u001b[Kv->ne[0] == S\u001b[m\u001b[K && v->ne[1] == 1 && v->ne[2] == H && v->ne[3] == T);\n",
      "      |                 \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:156:60:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  156 |     GGML_ASSERT(v->ne[0] == S && v->ne[1] == 1 && \u001b[01;35m\u001b[Kv->ne[2] == H\u001b[m\u001b[K && v->ne[3] == T);\n",
      "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:156:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  156 | (v->ne[0] == S && v->ne[1] == 1 && v->ne[2] == H && \u001b[01;35m\u001b[Kv->ne[3] == T\u001b[m\u001b[K);\n",
      "      |                                                     \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:157:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  157 |     GGML_ASSERT(\u001b[01;35m\u001b[Kr->ne[0] == S\u001b[m\u001b[K && r->ne[1] == 1 && r->ne[2] == H && r->ne[3] == T);\n",
      "      |                 \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:157:60:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  157 |     GGML_ASSERT(r->ne[0] == S && r->ne[1] == 1 && \u001b[01;35m\u001b[Kr->ne[2] == H\u001b[m\u001b[K && r->ne[3] == T);\n",
      "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:157:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kconst size_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  157 | (r->ne[0] == S && r->ne[1] == 1 && r->ne[2] == H && \u001b[01;35m\u001b[Kr->ne[3] == T\u001b[m\u001b[K);\n",
      "      |                                                     \u001b[01;35m\u001b[K~~~~~~~~~^~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_operators_wkv_v5.inc:158:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "  158 |     GGML_ASSERT(\u001b[01;35m\u001b[Kggml_nelements(state) == S * S * H\u001b[m\u001b[K);\n",
      "      |                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/ggml/include/ggml/ggml.h:246:15:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KGGML_ASSERT\u001b[m\u001b[K’\n",
      "  246 |         if (!(\u001b[01;36m\u001b[Kx\u001b[m\u001b[K)) { \\\n",
      "      |               \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv.cpp:91\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_gpu_offload.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool rwkv_gpu_offload_layers(rwkv_context*, uint32_t)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_gpu_offload.inc:58:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kctx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
      "   58 | bool rwkv_gpu_offload_layers(\u001b[01;35m\u001b[Kstruct rwkv_context * ctx\u001b[m\u001b[K, const uint32_t n_layers) {\n",
      "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_gpu_offload.inc:58:72:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kn_layers\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
      "   58 | l rwkv_gpu_offload_layers(struct rwkv_context * ctx, \u001b[01;35m\u001b[Kconst uint32_t n_layers\u001b[m\u001b[K) {\n",
      "      |                                                      \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "In file included from \u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv.cpp:93\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_eval.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool rwkv_eval_sequence_in_chunks(rwkv_context*, const uint32_t*, size_t, size_t, const float*, float*, float*)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/rwkv_eval.inc:135:45:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst uint32_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst unsigned int*\u001b[m\u001b[K’} to type ‘\u001b[01m\u001b[Kuint32_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kunsigned int*\u001b[m\u001b[K’} casts away qualifiers [\u001b[01;35m\u001b[K-Wcast-qual\u001b[m\u001b[K]\n",
      "  135 |     uint32_t * tokens_offset = (uint32_t *) \u001b[01;35m\u001b[Ktokens\u001b[m\u001b[K;\n",
      "      |                                             \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
      "[ 18%] \u001b[32m\u001b[1mLinking CXX shared library librwkv.so\u001b[0m\n",
      "[ 18%] Built target rwkv\n",
      "[ 22%] \u001b[32mBuilding C object tests/CMakeFiles/test_ggml_basics.dir/test_ggml_basics.c.o\u001b[0m\n",
      "[ 27%] \u001b[32m\u001b[1mLinking C executable ../bin/test_ggml_basics\u001b[0m\n",
      "[ 27%] Built target test_ggml_basics\n",
      "[ 31%] \u001b[32mBuilding C object tests/CMakeFiles/test_quantized_matmul_on_gpu.dir/test_quantized_matmul_on_gpu.c.o\u001b[0m\n",
      "[ 36%] \u001b[32m\u001b[1mLinking C executable ../bin/test_quantized_matmul_on_gpu\u001b[0m\n",
      "[ 36%] Built target test_quantized_matmul_on_gpu\n",
      "[ 40%] \u001b[32mBuilding C object tests/CMakeFiles/test_tiny_rwkv.dir/test_tiny_rwkv.c.o\u001b[0m\n",
      "In file included from \u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_tiny_rwkv.c:7\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktest_model\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:68:65:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K-Wdouble-promotion\u001b[m\u001b[K]\n",
      "   68 |  fprintf(stderr, \"Serial difference sum: %f, expected %f\\n\", \u001b[01;35m\u001b[Kdiff_sum\u001b[m\u001b[K, max_diff);\n",
      "      |                                                              \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:68:75:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K-Wdouble-promotion\u001b[m\u001b[K]\n",
      "   68 | printf(stderr, \"Serial difference sum: %f, expected %f\\n\", diff_sum, \u001b[01;35m\u001b[Kmax_diff\u001b[m\u001b[K);\n",
      "      |                                                                      \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "In file included from \u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_tiny_rwkv.c:7\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:83:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K-Wdouble-promotion\u001b[m\u001b[K]\n",
      "   83 | printf(stderr, \"Sequence difference sum: %f, expected %f\\n\", \u001b[01;35m\u001b[Kdiff_sum\u001b[m\u001b[K, max_diff);\n",
      "      |                                                              \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:83:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K-Wdouble-promotion\u001b[m\u001b[K]\n",
      "   83 | intf(stderr, \"Sequence difference sum: %f, expected %f\\n\", diff_sum, \u001b[01;35m\u001b[Kmax_diff\u001b[m\u001b[K);\n",
      "      |                                                                      \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_tiny_rwkv.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_tiny_rwkv.c:123:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K%s\u001b[m\u001b[K’ directive writing up to 127 bytes into a region of size 117 [\u001b[01;35m\u001b[K-Wformat-overflow=\u001b[m\u001b[K]\n",
      "  123 |             sprintf(dest_file_name, \"tiny-rwkv-%s-\u001b[01;35m\u001b[K%s\u001b[m\u001b[K.bin\", versions[i_version], \u001b[32m\u001b[Kdest_format\u001b[m\u001b[K);\n",
      "      |                                                   \u001b[01;35m\u001b[K^~\u001b[m\u001b[K                            \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/include/stdio.h:867\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_tiny_rwkv.c:3\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[K__builtin___sprintf_chk\u001b[m\u001b[K’ output 16 or more bytes (assuming 143) into a destination of size 128\n",
      "   36 |   return \u001b[01;36m\u001b[K__builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\u001b[m\u001b[K\n",
      "      |          \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "   37 | \u001b[01;36m\u001b[K      __bos (__s), __fmt, __va_arg_pack ())\u001b[m\u001b[K;\n",
      "      |       \u001b[01;36m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_tiny_rwkv.c:133:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K%s\u001b[m\u001b[K’ directive writing up to 127 bytes into a region of size 117 [\u001b[01;35m\u001b[K-Wformat-overflow=\u001b[m\u001b[K]\n",
      "  133 |             sprintf(dest_file_name, \"tiny-rwkv-%s-\u001b[01;35m\u001b[K%s\u001b[m\u001b[K.bin\", versions[i_version], \u001b[32m\u001b[Kdest_format\u001b[m\u001b[K);\n",
      "      |                                                   \u001b[01;35m\u001b[K^~\u001b[m\u001b[K                            \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/usr/include/stdio.h:867\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_tiny_rwkv.c:3\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[K__builtin___sprintf_chk\u001b[m\u001b[K’ output 16 or more bytes (assuming 143) into a destination of size 128\n",
      "   36 |   return \u001b[01;36m\u001b[K__builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\u001b[m\u001b[K\n",
      "      |          \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "   37 | \u001b[01;36m\u001b[K      __bos (__s), __fmt, __va_arg_pack ())\u001b[m\u001b[K;\n",
      "      |       \u001b[01;36m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "[ 45%] \u001b[32m\u001b[1mLinking C executable ../bin/test_tiny_rwkv\u001b[0m\n",
      "[ 45%] Built target test_tiny_rwkv\n",
      "[ 50%] \u001b[32mBuilding C object tests/CMakeFiles/test_quantization_format_compatibility.dir/test_quantization_format_compatibility.c.o\u001b[0m\n",
      "In file included from \u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_quantization_format_compatibility.c:7\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktest_model\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:68:65:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K-Wdouble-promotion\u001b[m\u001b[K]\n",
      "   68 |  fprintf(stderr, \"Serial difference sum: %f, expected %f\\n\", \u001b[01;35m\u001b[Kdiff_sum\u001b[m\u001b[K, max_diff);\n",
      "      |                                                              \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:68:75:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K-Wdouble-promotion\u001b[m\u001b[K]\n",
      "   68 | printf(stderr, \"Serial difference sum: %f, expected %f\\n\", diff_sum, \u001b[01;35m\u001b[Kmax_diff\u001b[m\u001b[K);\n",
      "      |                                                                      \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "In file included from \u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_quantization_format_compatibility.c:7\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:83:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K-Wdouble-promotion\u001b[m\u001b[K]\n",
      "   83 | printf(stderr, \"Sequence difference sum: %f, expected %f\\n\", \u001b[01;35m\u001b[Kdiff_sum\u001b[m\u001b[K, max_diff);\n",
      "      |                                                              \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/logit_difference_validator.inc:83:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K-Wdouble-promotion\u001b[m\u001b[K]\n",
      "   83 | intf(stderr, \"Sequence difference sum: %f, expected %f\\n\", diff_sum, \u001b[01;35m\u001b[Kmax_diff\u001b[m\u001b[K);\n",
      "      |                                                                      \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
      "\n",
      "[ 54%] \u001b[32m\u001b[1mLinking C executable ../bin/test_quantization_format_compatibility\u001b[0m\n",
      "[ 54%] Built target test_quantization_format_compatibility\n",
      "[ 59%] \u001b[32mBuilding C object tests/CMakeFiles/test_logit_calculation_skipping.dir/test_logit_calculation_skipping.c.o\u001b[0m\n",
      "[ 63%] \u001b[32m\u001b[1mLinking C executable ../bin/test_logit_calculation_skipping\u001b[0m\n",
      "[ 63%] Built target test_logit_calculation_skipping\n",
      "[ 68%] \u001b[32mBuilding C object tests/CMakeFiles/test_eval_sequence_in_chunks.dir/test_eval_sequence_in_chunks.c.o\u001b[0m\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_eval_sequence_in_chunks.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktest_on_prompt\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K/workspace/rwkv.cpp/tests/test_eval_sequence_in_chunks.c:33:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Ksize_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
      "   33 |     for (int i = 0; i \u001b[01;35m\u001b[K<\u001b[m\u001b[K prompt_length; i++) {\n",
      "      |                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "[ 72%] \u001b[32m\u001b[1mLinking C executable ../bin/test_eval_sequence_in_chunks\u001b[0m\n",
      "[ 72%] Built target test_eval_sequence_in_chunks\n",
      "[ 77%] \u001b[32mBuilding C object tests/CMakeFiles/test_context_cloning.dir/test_context_cloning.c.o\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking C executable ../bin/test_context_cloning\u001b[0m\n",
      "[ 81%] Built target test_context_cloning\n",
      "[ 86%] \u001b[32mBuilding C object extras/CMakeFiles/rwkv_cpu_info.dir/cpu_info.c.o\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking C executable ../bin/rwkv_cpu_info\u001b[0m\n",
      "[ 90%] Built target rwkv_cpu_info\n",
      "[ 95%] \u001b[32mBuilding C object extras/CMakeFiles/rwkv_quantize.dir/quantize.c.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking C executable ../bin/rwkv_quantize\u001b[0m\n",
      "[100%] Built target rwkv_quantize\n"
     ]
    }
   ],
   "source": [
    "!cmake .\n",
    "!cmake --build . --config Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7841ad1-b171-4de7-a896-775332535dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth\n",
      "Detected RWKV v5.2\n",
      "Writing emb.weight, shape torch.Size([65536, 4096]), type torch.float16\n",
      "Writing blocks.0.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.ln0.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.ln0.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.0.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.0.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.0.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.0.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.0.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.0.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.0.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.0.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.0.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.0.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.1.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.1.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.1.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.1.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.1.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.1.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.1.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.1.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.1.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.1.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.1.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.2.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.2.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.2.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.2.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.2.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.2.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.2.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.2.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.2.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.2.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.2.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.3.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.3.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.3.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.3.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.3.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.3.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.3.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.3.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.3.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.3.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.3.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.4.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.4.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.4.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.4.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.4.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.4.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.4.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.4.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.4.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.4.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.4.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.5.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.5.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.5.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.5.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.5.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.5.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.5.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.5.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.5.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.5.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.5.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.6.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.6.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.6.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.6.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.6.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.6.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.6.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.6.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.6.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.6.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.6.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.7.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.7.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.7.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.7.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.7.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.7.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.7.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.7.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.7.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.7.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.7.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.8.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.8.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.8.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.8.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.8.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.8.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.8.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.8.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.8.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.8.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.8.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.9.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.9.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.9.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.9.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.9.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.9.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.9.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.9.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.9.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.9.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.9.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.10.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.10.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.10.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.10.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.10.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.10.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.10.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.10.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.10.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.10.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.10.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.11.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.11.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.11.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.11.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.11.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.11.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.11.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.11.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.11.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.11.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.11.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.12.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.12.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.12.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.12.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.12.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.12.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.12.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.12.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.12.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.12.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.12.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.13.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.13.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.13.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.13.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.13.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.13.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.13.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.13.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.13.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.13.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.13.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.14.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.14.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.14.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.14.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.14.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.14.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.14.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.14.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.14.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.14.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.14.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.15.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.15.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.15.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.15.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.15.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.15.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.15.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.15.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.15.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.15.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.15.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.16.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.16.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.16.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.16.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.16.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.16.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.16.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.16.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.16.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.16.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.16.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.17.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.17.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.17.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.17.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.17.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.17.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.17.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.17.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.17.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.17.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.17.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.18.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.18.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.18.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.18.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.18.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.18.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.18.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.18.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.18.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.18.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.18.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.19.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.19.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.19.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.19.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.19.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.19.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.19.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.19.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.19.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.19.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.19.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.20.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.20.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.20.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.20.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.20.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.20.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.20.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.20.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.20.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.20.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.20.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.21.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.21.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.21.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.21.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.21.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.21.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.21.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.21.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.21.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.21.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.21.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.22.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.22.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.22.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.22.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.22.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.22.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.22.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.22.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.22.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.22.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.22.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.23.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.23.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.23.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.23.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.23.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.23.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.23.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.23.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.23.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.23.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.23.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.24.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.24.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.24.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.24.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.24.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.24.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.24.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.24.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.24.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.24.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.24.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.25.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.25.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.25.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.25.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.25.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.25.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.25.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.25.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.25.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.25.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.25.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.26.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.26.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.26.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.26.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.26.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.26.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.26.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.26.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.26.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.26.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.26.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.27.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.27.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.27.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.27.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.27.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.27.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.27.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.27.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.27.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.27.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.27.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.28.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.28.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.28.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.28.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.28.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.28.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.28.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.28.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.28.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.28.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.28.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.29.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.29.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.29.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.29.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.29.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.29.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.29.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.29.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.29.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.29.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.29.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.30.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.30.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.30.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.30.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.30.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.30.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.30.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.30.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.30.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.30.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.30.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing blocks.31.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.att.time_mix_g, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.att.time_decay, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.31.att.time_faaaa, shape torch.Size([64, 64, 1]), type torch.float32\n",
      "Writing blocks.31.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.31.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.31.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.31.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.31.att.gate.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.31.att.ln_x.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.att.ln_x.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
      "Writing blocks.31.ffn.key.weight, shape torch.Size([14336, 4096]), type torch.float16\n",
      "Writing blocks.31.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
      "Writing blocks.31.ffn.value.weight, shape torch.Size([4096, 14336]), type torch.float16\n",
      "Writing ln_out.weight, shape torch.Size([4096]), type torch.float32\n",
      "Writing ln_out.bias, shape torch.Size([4096]), type torch.float32\n",
      "Writing head.weight, shape torch.Size([65536, 4096]), type torch.float16\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!python python/convert_pytorch_to_ggml.py ../RWKV-v5-Eagle-World-7B-v2-20240128-ctx4096.pth RWKV-v5-Eagle-World-7B-v2.bin FP16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1624ede1-a694-455e-a68f-f4406fe4b1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from 'RWKV-v5-Eagle-World-7B-v2.bin'\n",
      "                     emb.weight - [ 4096, 65536,     1], type =   FP16 size =  512.000 MB\n",
      "            blocks.0.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.0.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.0.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.0.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.0.ln0.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.0.ln0.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.0.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "        blocks.0.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.0.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.0.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.0.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.0.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.0.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.0.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      " blocks.0.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.0.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.064 0.066 0.079 0.070 0.058 0.057 0.055 0.054 0.054 0.054 0.066 \n",
      "            blocks.1.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.1.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.1.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.1.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.1.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
      "        blocks.1.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.1.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.1.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.1.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.1.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.1.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.1.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      " blocks.1.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.1.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "            blocks.2.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.2.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.2.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.2.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.2.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "        blocks.2.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.2.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.2.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.2.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.2.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.2.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.2.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      " blocks.2.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.062 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.2.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "            blocks.3.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.3.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.3.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.3.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.3.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
      "        blocks.3.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.3.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.3.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.053 0.065 \n",
      "       blocks.3.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.3.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.3.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.3.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      " blocks.3.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.062 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.3.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "            blocks.4.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.4.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.4.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.4.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.4.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "        blocks.4.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
      "      blocks.4.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.058 0.056 0.054 0.054 0.054 0.066 \n",
      "     blocks.4.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
      "       blocks.4.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.4.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.4.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.4.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      " blocks.4.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.4.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "            blocks.5.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.5.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.5.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.5.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.5.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "        blocks.5.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.5.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.5.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.5.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.5.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.5.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.5.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      " blocks.5.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "      blocks.5.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "            blocks.6.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.6.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.6.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.6.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.6.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "        blocks.6.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.058 0.056 0.055 0.054 0.053 0.066 \n",
      "      blocks.6.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.6.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.6.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.6.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.6.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.6.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      " blocks.6.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "      blocks.6.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "            blocks.7.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.7.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.7.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.7.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.7.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "        blocks.7.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.058 0.056 0.055 0.054 0.053 0.066 \n",
      "      blocks.7.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
      "     blocks.7.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
      "       blocks.7.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.7.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.7.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.7.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      " blocks.7.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.7.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "            blocks.8.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.8.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.8.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.8.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.8.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "        blocks.8.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.8.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.8.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.8.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.8.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.8.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.8.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      " blocks.8.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "      blocks.8.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "            blocks.9.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.9.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "            blocks.9.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "              blocks.9.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      " blocks.9.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "        blocks.9.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.9.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.9.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.9.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.9.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "         blocks.9.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.9.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      " blocks.9.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "      blocks.9.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.10.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.10.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.10.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.10.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.10.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.10.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.10.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.10.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
      "      blocks.10.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.10.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.10.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.10.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.10.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.10.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.11.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.11.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.11.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.11.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.11.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.11.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
      "     blocks.11.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.11.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.11.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.11.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.11.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.11.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.11.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.11.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.12.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.12.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.12.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.12.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.12.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.12.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.12.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.12.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "      blocks.12.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.12.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.12.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.12.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.12.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.12.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.13.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.13.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.13.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.13.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.13.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.13.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.13.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.13.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.13.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "      blocks.13.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.13.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.13.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.13.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.13.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.14.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.14.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.14.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.14.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.14.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.14.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.14.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.14.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.14.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.14.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.14.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.14.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.14.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.14.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.15.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.15.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.15.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.15.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.15.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.15.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.15.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.15.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.15.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.15.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.15.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.15.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.15.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.15.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.16.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.16.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.16.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.16.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.16.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.16.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.16.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.16.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.16.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.16.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.16.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.16.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.16.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.16.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.17.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.17.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.17.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.17.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.17.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.17.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.17.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.17.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.17.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.17.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.17.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.17.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.17.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.17.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.18.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.18.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.18.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.18.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.18.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.18.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.18.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.18.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.18.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.18.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.18.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.18.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.18.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.18.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.19.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.19.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.19.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.19.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.19.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.19.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.19.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.19.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.19.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.19.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.19.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.19.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.19.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.19.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.20.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.20.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.20.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.20.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.20.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.20.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.20.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.20.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.20.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.20.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.20.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.20.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.20.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.20.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.21.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.21.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.21.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.21.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.21.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.21.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.21.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.21.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.21.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.21.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.21.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.21.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.21.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.21.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.22.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.22.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.22.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.22.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.22.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.22.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.22.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.22.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.22.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.22.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.22.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.22.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.22.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.22.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.23.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.23.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.23.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.23.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.23.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.23.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.23.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.23.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.23.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.23.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.23.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.23.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.23.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.23.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.24.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.24.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.24.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.24.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.24.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.24.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.24.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.24.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.055 0.054 0.054 0.053 0.066 \n",
      "      blocks.24.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.24.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.24.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.24.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.24.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.24.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.25.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.25.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.25.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.25.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.25.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.25.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
      "     blocks.25.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.25.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
      "      blocks.25.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.25.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.25.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.25.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.25.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "     blocks.25.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.26.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.26.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.26.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.26.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.26.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.26.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.26.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "    blocks.26.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.26.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.26.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.26.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.26.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.26.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.26.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.27.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.27.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.27.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.27.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.27.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.27.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.27.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.27.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.27.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.27.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.27.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.27.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.27.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.27.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.28.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.28.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.28.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.28.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.28.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.28.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.28.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "    blocks.28.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.28.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.28.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.28.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.28.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.28.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.28.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.29.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.29.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.29.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.29.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.29.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.29.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.29.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.29.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.29.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.29.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.29.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.29.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.29.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.29.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.30.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.30.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.30.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.30.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.30.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "       blocks.30.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.30.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.30.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.30.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "      blocks.30.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.30.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.30.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.30.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.30.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "           blocks.31.ln1.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.31.ln1.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "           blocks.31.ln2.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "             blocks.31.ln2.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.att.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.att.time_mix_v - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.att.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.att.time_mix_g - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.att.time_decay - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.att.time_faaaa - [    1,    64,    64], type =   FP32 size =    0.016 MB\n",
      "blocks.31.att.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "       blocks.31.att.key.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "     blocks.31.att.value.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "    blocks.31.att.output.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.31.att.gate.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "      blocks.31.att.ln_x.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "        blocks.31.att.ln_x.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.ffn.time_mix_k - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.ffn.time_mix_r - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "       blocks.31.ffn.key.weight - [ 4096, 14336,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
      "blocks.31.ffn.receptance.weight - [ 4096,  4096,     1], type =   FP16 quantizing... size =    32.00 MB ->    12.00 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
      "     blocks.31.ffn.value.weight - [14336,  4096,     1], type =   FP16 quantizing... size =   112.00 MB ->    42.00 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
      "                  ln_out.weight - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "                    ln_out.bias - [ 4096,     1,     1], type =   FP32 size =    0.016 MB\n",
      "                    head.weight - [ 4096, 65536,     1], type =   FP16 size =  512.000 MB\n",
      "original size     = 14343.06 MB\n",
      "quantized size    =  6023.06 MB\n",
      "compression ratio =     2.38\n",
      "hist: \n",
      "Done\n",
      "0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 "
     ]
    }
   ],
   "source": [
    "!python python/quantize.py RWKV-v5-Eagle-World-7B-v2.bin RWKV-v5-Eagle-World-7B-v2-Q5_1.bin Q5_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed8325ff-4a23-45ce-a5d0-e39dbaa02ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading World v20230424 tokenizer\n",
      "One upon a time, a beautiful woman named Zelda was given a gift from the gods. She was told that this gift would give her great power, but she must never use it"
     ]
    }
   ],
   "source": [
    "!python python/inference_example.py RWKV-v5-Eagle-World-7B-v2.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9e94f36-d780-4a6a-be85-2648789aeff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading World v20230424 tokenizer\n",
      "One upon a time, there was a poor man who lived in a small house. The man had a dog. The dog was his best friend. They played together every day.\n"
     ]
    }
   ],
   "source": [
    "!python python/inference_example.py RWKV-v5-Eagle-World-7B-v2-Q5_1.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015d5a5-ae51-49c8-9b8a-2a565ab8253f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
