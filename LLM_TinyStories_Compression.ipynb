{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubpradeep/notebooks/blob/main/LLM_TinyStories_Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYJoyfllAVBS"
      },
      "outputs": [],
      "source": [
        "!pip install rouge\n",
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
        "# install additional dependencies needed for training\n",
        "!pip install rouge-score tensorboard py7zr\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0sbCRLMBFWV",
        "outputId": "ddab1f1d-7e08-4fc1-abeb-a635fb05abaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "4nh8ZX_9wHpr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device('cuda')\n"
      ],
      "metadata": {
        "id": "5eJOTa2RwIT2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gKae99qA93N",
        "outputId": "7effc148-5cfb-44c9-d9f0-3494b617f95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, scary dog. The dog barked and growled at her. Lily was scared and ran away.\n",
            "\n",
            "Later that day, Lily's mom asked her to help with the laundry. Lily was happy to help and started to sort the clothes. She found a shirt that was too small for her, but she was proud to find it.\n",
            "\n",
            "As the sun started to set, Lily's mom called her inside. She took off her wet clothes and put them in the dryer. Lily was happy to be inside where it was warm and cozy. She went to bed that night feeling proud of herself for helping with the laundry.\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "torch.set_default_device('cuda')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-33M\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-33M\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "inputs = tokenizer('''once upon a time''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Agq-4D9twJjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oKNWfV0rBBcU"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class LowRankConfig:\n",
        "    rank:int\n",
        "    target_modules: list[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frxqJod8Ba7H",
        "outputId": "8d9d83ff-5d31-4e79-f76e-6fe8b87f336d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoForCausalLM(\n",
              "  (transformer): GPTNeoModel(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(2048, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-3): 4 x GPTNeoBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTNeoAttention(\n",
              "          (attention): GPTNeoSelfAttention(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPTNeoMLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzAD12fkBizH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Tm9bmIcPBz23"
      },
      "outputs": [],
      "source": [
        "#low rank decomposition of SelfAttention Key, Query and Value Matrices\n",
        "config = LowRankConfig(\n",
        "    rank= 384,\n",
        "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "n09Mv2V6B2ra"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from dataclasses import dataclass\n",
        "from torch.nn import functional as F\n",
        "class LowRankLayer(nn.Module):\n",
        "    \"\"\"given a linear layer find low rank decomposition\"\"\"\n",
        "    def __init__(self, rank, full_rank_layer):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "\n",
        "        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n",
        "        S_diag = torch.diag(S)\n",
        "        self.U = U[:, :self.rank]\n",
        "        self.S = S_diag[:self.rank, :self.rank]\n",
        "        self.Vh = Vh[:self.rank, :]\n",
        "\n",
        "    def forward(self, x):\n",
        "        aprox_weight_matrix = self.U @ self.S @ self.Vh\n",
        "        output = F.linear(x, aprox_weight_matrix)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "hXECniy_B4ZP"
      },
      "outputs": [],
      "source": [
        "#find the module that ends target suffix\n",
        "def get_submodules(model, key):\n",
        "    parent = model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n",
        "    target_name = key.split(\".\")[-1]\n",
        "    target = model.get_submodule(key)\n",
        "    return parent, target, target_name\n",
        "\n",
        "# this function replaces a target layer with low rank layer\n",
        "def recursive_setattr(obj, attr, value):\n",
        "    attr = attr.split('.', 1)\n",
        "    if len(attr) == 1:\n",
        "        setattr(obj, attr[0], value)\n",
        "    else:\n",
        "        recursive_setattr(getattr(obj, attr[0]), attr[1], value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FnYiv17sCAqk"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "model_lr = copy.deepcopy(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "LeJxtZ82CE-x"
      },
      "outputs": [],
      "source": [
        "for key, module in model.named_modules():\n",
        "    target_module_found = any(key.endswith(\".\" + target_key) for target_key in config.target_modules)\n",
        "    if target_module_found:\n",
        "        low_rank_layer = LowRankLayer(config.rank, module)\n",
        "        #replace target layer with low rank layer\n",
        "        recursive_setattr(model_lr, key, low_rank_layer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "hX32OjtOWxbh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "CJGGyGNvCNdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ebc59a-1bca-47c0-bcee-33395a783785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 68514048 || all params: 68514048 || trainable%: 100.0\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fLjIyz-WuR8",
        "outputId": "f0e7a8af-6b26-42c9-e27b-bf2f677627d8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 61436160 || all params: 61436160 || trainable%: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1-61436160/68514048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp5S4gBNyJPf",
        "outputId": "3232efc3-002a-4058-89bc-cb2a64777ccb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10330564616471061"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"model\", from_pt=True)\n"
      ],
      "metadata": {
        "id": "Cs1r1yqjyUL1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lr.save_pretrained(\"model_lr\", from_pt=True)\n"
      ],
      "metadata": {
        "id": "oE5UFsx2W3a6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh model/pytorch_model.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "FZRdTj2GXBXK",
        "outputId": "d1698090-5950-4ddc-c8bd-d350b99a3773"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-e20ae2a69294>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls -lh model/pytorch_model.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh model_lr/pytorch_model.bin"
      ],
      "metadata": {
        "id": "sPsvu6eeXIq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1-2.1/2.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qVTIg2rycKd",
        "outputId": "49a09e7c-0bee-49c4-e0ff-566ff7258e7f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2222222222222222"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.set_default_device('cuda')\n",
        "inputs = tokenizer('''once upon a time''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model_lr.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUoMIJ7lYGOo",
        "outputId": "3cfd7758-169a-4262-adfb-9b8998b4b7e6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time there was a little girl called Lucy. She was very curious and loved to explore.\n",
            "\n",
            "One day, Lucy went to the park. She saw a big tree with a big, long rope. She wanted to climb it, so she did. She felt very brave and happy.\n",
            "\n",
            "Suddenly, she heard a voice. It was a little bird. The bird said, \"Be careful, Lucy! You must be careful when you climb the rope.\"\n",
            "\n",
            "Lucy was very careful. She said, \"Thank you for warning me. I will be careful.\"\n",
            "\n",
            "The bird said, \"You're welcome. I'm glad you're careful. Now, let's go explore the park together.\"\n",
            "\n",
            "Lucy and the bird had lots of fun playing together. They had a lot of fun.\n",
            "\n",
            "The end.\n",
            "<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.set_default_device('cuda')\n",
        "inputs = tokenizer('''A goat was''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model_lr.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk8MZ9jPztzj",
        "outputId": "b9c1962f-cd6e-4784-8130-b2b1346b2294"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A goat was eating the grass. It was very hungry. It saw the children and started to run towards them. The children were scared and ran away. The goat was left alone. The goat was still hungry and looked for something else to eat. It was very hungry and sad.\n",
            "\n",
            "The goat saw a small house with a garden. It went inside and saw a big bowl of food. It smelled something delicious inside the house. The goat was very happy and started to eat the food. It ate and ate until it was full. The goat ate all the food in the garden. It was very full and happy.\n",
            "\n",
            "But then, the goat heard a loud noise. It turned around and saw a big storm coming. The goat was scared and tried to run away. But it was too late. The storm was too strong and the goat got hurt. The goat was very sad and wished it had never come to the house. It wished it had never left it alone.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYvCDdi9Dtd4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5wJOBTYomf3eIIQzl/hO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}